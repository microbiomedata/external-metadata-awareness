# Analysis for MONDO paper
The intent is to have all data figures in paper generated by Jupyter.

The analysis should be reproducible. We include _obographjson_ files of each 
ontology in the repo.

In theory everything can be executed from here - either standard python install 
with `requirements.txt`, or _docker2repo_, or _jupyterhub_.

## TODO's
- [x] 1.Get all `sources/` needed, making sure `make all` works.
- [ ] 2.More figures.
- [ ] 3.Switch from matplotlib to seaborn.
- [ ] 4.In 'Axiom analysis', how is this different than the build process? 
  Where does the analysis come in?

## Setup and running
### Setup steps
1. Install [`robot`](http://robot.obolibrary.org/).
2. Download [`sources.zip`](https://drive.google.com/drive/folders/17o8gvQlL8aU1fOoQP3DWayqsHQET85pp) and unzip its contents into the `sources/` directory.
3. Run `make all`.
4. Make a python virtual environment & activate it 
   ([1](https://realpython.com/lessons/creating-virtual-environment/), 
   [2](https://towardsdatascience.com/virtual-environments-for-absolute-beginners-what-is-it-and-how-to-create-one-examples-a48da8982d4b))
5. Run `pip install -r requirements.txt`

### Running notebooks
1. Run `jupyter notebook`
2. A browser tab will open. Select the notebook you want to view.
3. Click 'Run' to execute single cells, or run the whole thing by pressing the 
   'fast-forward icon' button.

## Analyses
### Axiom analysis
The complete axiom analysis can be run using

```
make all
``` 

This will run the full pipeline for all ontologies in the `$SOURCE_IDS` variable. 
You can also pass that one in like so:

```
make SOURCE_IDS="omim doid" all
```

## Mondo Xref coverage analysis (for Mondo paper) 
### Updating sources
1. If you have to update the sources, run `make update_sources`. This will obtain latest versions of all ontologies except SNOMED (which has to be manually added if you have the legal rights).
2. If you want to download the latest zipped dump of the sources, run `make SOURCE_DATA_URL="http://some.location/somedata.zip prepare_sources`.
3. You can then run `make paper_analysis -B` to generate the relevant JSON files that constitute the preprocessed _versions_ of the ontologies for our work. Preprocessing involves: 1) subsetting to the relevant branches of the ontology. 2) fixing some known issues with xrefs, 3) transforming to OBO Graphs Json

## ICD10CM Exclusions
### Setup
1. Download `icd10cm_excluded_term_ranges_intensional.csv` from [GoogleDrive folder](https://drive.google.com/drive/folders/18L5UHVSspPqOyu2k5wg2GDXgsJE2Dy3i) into `preparation/`
2. Download `icd10cm.ttl` from [GoogleDrive folder](https://drive.google.com/drive/folders/18L5UHVSspPqOyu2k5wg2GDXgsJE2Dy3i) into `preparation/`
3. Make sure that `robot` is installed on your computer. Will not work by running through docker container. 
### Running
Run: `make icd10cm-exclusions-list`
