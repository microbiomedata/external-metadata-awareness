{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:32.822904Z",
     "start_time": "2025-03-26T20:54:30.080534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from linkml_runtime import SchemaView\n",
    "from oaklib import get_adapter\n",
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm  # Import tqdm.notebook for Jupyter Notebook progress bars"
   ],
   "id": "1e7d0dbb1605ea7d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:32.830478Z",
     "start_time": "2025-03-26T20:54:32.826965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys.path.append('../..')  # Assuming notebooks/ is at the root level\n",
    "import core # appears as if there's a problem in PyCharm Jupyter, but it still works\n"
   ],
   "id": "c533124cd6623bd6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:32.898056Z",
     "start_time": "2025-03-26T20:54:32.895402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# todo there is probably some duplicative code in here\n",
    "#   either similar functions for extracting stuff from NCBI Biosamples\n",
    "#     which maybe should go in the general flatterer\n",
    "#   or functionality that is already solved in the DuckDB dumper\n",
    "#     see also external_metadata_awareness/biosamples_mongodb_to_duckdb.py\n",
    "#   or the for blocks that find canonical labels and detect obsolescence\n",
    "#     which is not being applied to NCBI records yet\n",
    "#       we have addressed that in multiple other places :-( with OAK annotation\n",
    "#         which requires followup filtering/curation"
   ],
   "id": "1e26c95ed854605c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Flattening isn't perfect for the submission portal biosamples\n",
    "\n",
    "Am I trying to combine data from multiple templates? I don't see and EMSL or JGI slots... multiple MIxS-specified templates?\n",
    "\n",
    "- analysis_type[0]\n",
    "- analysis_type[1]\n",
    "- analysis_type[2]\n",
    "- analysis_type[3]\n",
    "- analysis_type[4]\n",
    "- analysis_type[5]\n",
    "- analysis_type[6]\n",
    "\n",
    "Why are these arrays but only with one item?\n",
    "\n",
    "- light_type[0]\n",
    "- tillage[0]\n",
    "\n",
    "columns are also present for the field that contains the list\n",
    "\n",
    "- analysis_type\n",
    "- light_type\n",
    "- tillage\n",
    "\n",
    "No template column... suggests that the templates are being combined\n",
    "The submission portal biosamples flattening doesn't scrutinize the environmental triad values the way other flatteners do.\n",
    "\n",
    "See https://data.microbiomedata.org/api/metadata_submission/mixs_report\n",
    "for an \"in (extension-appropriate?) submission-schema enum/valueset\" checker"
   ],
   "id": "24d84f2d62263847"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:32.946045Z",
     "start_time": "2025-03-26T20:54:32.943463Z"
    }
   },
   "cell_type": "code",
   "source": "# todo there are some confusing variables in here like emp500_ncbi_biosamples and ncbi_emp500_biosamples",
   "id": "c22f16167168ab7a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:32.993042Z",
     "start_time": "2025-03-26T20:54:32.990757Z"
    }
   },
   "cell_type": "code",
   "source": "NMDC_RUNTIME_API_BASE_URL = \"https://api.microbiomedata.org/nmdcschema/\"",
   "id": "6e347841c116977f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.045012Z",
     "start_time": "2025-03-26T20:54:33.042394Z"
    }
   },
   "cell_type": "code",
   "source": "nmdc_schema_url = \"https://raw.githubusercontent.com/microbiomedata/nmdc-schema/refs/heads/main/nmdc_schema/nmdc_materialized_patterns.yaml\"",
   "id": "178cd6574f83395",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.086658Z",
     "start_time": "2025-03-26T20:54:33.084102Z"
    }
   },
   "cell_type": "code",
   "source": "ontology_list = [\"envo\", \"pato\", \"uberon\"]\n",
   "id": "4ba7d0043ea7c95",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.132683Z",
     "start_time": "2025-03-26T20:54:33.129927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# there are other slots ontolgy terms too\n",
    "\n",
    "to_label_check = [\n",
    "    'env_broad_scale.id', 'env_local_scale.id', 'env_medium.id',\n",
    "    'env_broad_scale.term.id', 'env_local_scale.term.id', 'env_medium.term.id',\n",
    "    'envoBroadScale.id', 'envoLocalScale.id', 'envoLocalScale.id',\n",
    "]\n",
    "\n",
    "# the flattened NCMDC biosamples LOD has fields like \"env_broad_scale.term.id\"\n",
    "# the flattened NCMDC biosamples LOD has fields like \"env_broad_scale.id\""
   ],
   "id": "290af6092acdeead",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "add checking of other ontologies\n",
    "build an obsolete term cache"
   ],
   "id": "510c5bcd9527810d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.178457Z",
     "start_time": "2025-03-26T20:54:33.175944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # see also\n",
    "# #   notebooks/environmental_context_value_sets/generate_voting_sheet.ipynb\n",
    "#\n",
    "# with open(envo_label_cache_json, \"r\", encoding=\"utf-8\") as file:\n",
    "#     envo_label_cache = json.load(file)"
   ],
   "id": "fc2bb3105c378e31",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.229697Z",
     "start_time": "2025-03-26T20:54:33.222020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to the local MongoDB instance (default connection)\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Connect to your local MongoDB"
   ],
   "id": "b797dca7befa2a4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.271468Z",
     "start_time": "2025-03-26T20:54:33.268678Z"
    }
   },
   "cell_type": "code",
   "source": "submissions_db = client['misc_metadata']",
   "id": "defd92643b12c295",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.318587Z",
     "start_time": "2025-03-26T20:54:33.315807Z"
    }
   },
   "cell_type": "code",
   "source": "submissions_collection = submissions_db['nmdc_submissions']",
   "id": "84c47de30e2816b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.364443Z",
     "start_time": "2025-03-26T20:54:33.361751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GOLD_METADATA = \"gold_metadata_studies_with_samples\" # another MongoDB collection to check\n",
    "GOLD_METADATA = \"gold_metadata\" # following mongo-ncbi-loadbalancer's name which doesn't really indicate that the collection is a subset\n",
    "NCBI_METADATA = \"ncbi_metadata\"\n",
    "# todo add build notes here"
   ],
   "id": "173cba27c0187021",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.411114Z",
     "start_time": "2025-03-26T20:54:33.407744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "aid_tsv = \"alternative_identifiers_cheatsheet.tsv\"\n",
    "\n",
    "# biosample_chem_admin_tsv=\"flattened_biosample_chem_administration.tsv\"\n",
    "biosample_chem_admin_json = \"flattened_biosample_chem_administration.json\"\n",
    "\n",
    "# flattened_gold_emp500_biosamples_tsv = \"flattened_gold_emp500_biosamples.tsv\"\n",
    "# gold_emp500_biosample_contacts_tsv = \"gold_emp500_biosample_contacts.tsv\"\n",
    "\n",
    "# flattened_ncbi_emp500_biosamples_with_attributes_tsv= \"flattened_ncbi_emp500_biosamples.tsv\"\n",
    "# ncbi_emp500_all_attributes_tsv = \"ncbi_emp500_all_attributes.tsv\"\n",
    "# ncbi_emp500_biosample_ids_tsv = \"ncbi_emp500_biosample_ids.tsv\"\n",
    "# ncbi_emp500_biosample_models_tsv = \"ncbi_emp500_biosample_models.tsv\"\n",
    "\n",
    "# nmdc_submissions_biosamples_tsv = \"nmdc_submissions_biosamples.tsv\"\n",
    "nmdc_submissions_biosamples_json = \"nmdc_submissions_biosamples.json\"\n",
    "\n",
    "# scalar_biosamples_tsv = \"flattened_biosample.tsv\"\n",
    "scalar_biosamples_json = \"flattened_biosample.json\"\n",
    "\n",
    "# scalar_studies_tsv = \"flattened_study.tsv\"\n",
    "scalar_studies_json = \"flattened_study.json\"\n",
    "\n",
    "# study_credit_associations_tsv = \"flattened_study_has_credit_associations.tsv\"\n",
    "study_credit_associations_json = \"flattened_study_has_credit_associations.json\"\n",
    "\n",
    "# study_dois_tsv = \"flattened_study_associated_dois.tsv\"\n",
    "study_dois_json = \"flattened_study_associated_dois.json\""
   ],
   "id": "4a8eaaf0facfccab",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.458016Z",
     "start_time": "2025-03-26T20:54:33.455202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this code now globally processes NMDC MongDB contents and records from the submissions API\n",
    "\n",
    "# EMP500 would be a good scope for checking GOLD or NCBI records\n",
    "\n",
    "# see also notebooks/studies_exploration/emp_500_ng/emp500_ng.ipynb\n",
    "#   for illustration of how study ids were determined from text\n",
    "\n",
    "# will probably re-use some code from (see also)\n",
    "#   notebooks/mixs-slot-ranking/build_mixs_slot_rank_template.ipynb"
   ],
   "id": "79b4a9f57cff0066",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.504315Z",
     "start_time": "2025-03-26T20:54:33.501394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emp500_ncbi_project_accession = \"PRJEB42019\"\n",
    "emp500_gold_study_id = \"Gs0154244\""
   ],
   "id": "233dac7771507590",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.550923Z",
     "start_time": "2025-03-26T20:54:33.547160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_label_curie(text):\n",
    "    \"\"\"\n",
    "    Parses a string with an optional leading underscore, followed by a label and a CURIE inside square brackets.\n",
    "\n",
    "    Example input: \"________mediterranean savanna biome [ENVO:01000229]\"\n",
    "\n",
    "    :param text: The input string to parse\n",
    "    :return: A dictionary {'label': <label>, 'curie': <curie>} if successful, else None\n",
    "    \"\"\"\n",
    "    pattern = r\"^_*(?P<label>[^\\[\\]]+)\\s*\\[(?P<curie>[^\\[\\]]+)\\]$\"\n",
    "    match = re.match(pattern, text.strip())\n",
    "\n",
    "    if match:\n",
    "        return {\n",
    "            \"label\": match.group(\"label\").strip(),\n",
    "            \"curie\": match.group(\"curie\").strip()\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n"
   ],
   "id": "bd8c13d0b805e0d0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.597567Z",
     "start_time": "2025-03-26T20:54:33.594370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_label_cache(entities, adapter):\n",
    "    \"\"\"\n",
    "    Generates a label cache mapping CURIEs to their labels.\n",
    "\n",
    "    :param entities: List of ontology entities (CURIEs)\n",
    "    :param adapter: Ontology adapter to fetch labels\n",
    "    :return: Dictionary mapping CURIEs to labels\n",
    "    \"\"\"\n",
    "    label_cache = {}\n",
    "\n",
    "    for curie in entities:\n",
    "        label = adapter.label(curie)  # Fetch label for CURIE\n",
    "        if label:  # Only store if a valid label exists\n",
    "            label_cache[curie] = label\n",
    "\n",
    "    return label_cache\n"
   ],
   "id": "db6011d12e55fcba",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.644054Z",
     "start_time": "2025-03-26T20:54:33.640894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_ontology_adapters(ontology_names):\n",
    "    \"\"\"\n",
    "    Creates ontology adapters for the given ontology names.\n",
    "\n",
    "    :param ontology_names: List of ontology names (e.g., [\"envo\", \"pato\", \"uberon\"])\n",
    "    :return: Dictionary mapping ontology names to ontology adapters\n",
    "    \"\"\"\n",
    "    return {ontology: get_adapter(f\"sqlite:obo:{ontology}\") for ontology in ontology_names}\n"
   ],
   "id": "740f6ff8d7a43bd",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.693807Z",
     "start_time": "2025-03-26T20:54:33.690070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_ontology_labels(ontology_adapters):\n",
    "    \"\"\"\n",
    "    Loads ontology entity labels from multiple ontologies and aggregates their label caches.\n",
    "\n",
    "    :param ontology_adapters: Dictionary mapping ontology names to ontology adapters\n",
    "    :return: Aggregated label cache dictionary\n",
    "    \"\"\"\n",
    "    aggregated_label_cache = {}\n",
    "\n",
    "    for ontology, adapter in ontology_adapters.items():\n",
    "        entities = sorted(list(adapter.entities()))  # Fetch and sort entities\n",
    "        label_cache = generate_label_cache(entities, adapter)\n",
    "        aggregated_label_cache.update(label_cache)  # Merge into a single cache\n",
    "\n",
    "    return aggregated_label_cache"
   ],
   "id": "df2246541a67567d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.739817Z",
     "start_time": "2025-03-26T20:54:33.736761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_obsolete_terms(ontology_adapters):\n",
    "    \"\"\"\n",
    "    Identifies obsolete terms from multiple ontologies using their adapters.\n",
    "\n",
    "    :param ontology_adapters: Dictionary mapping ontology names to ontology adapters\n",
    "    :return: List of CURIEs for obsolete terms\n",
    "    \"\"\"\n",
    "    obsolete_curies = []\n",
    "\n",
    "    for adapter in ontology_adapters.values():\n",
    "        obsolete_curies.extend(adapter.obsoletes())  # Use ontology access kit function\n",
    "\n",
    "    return obsolete_curies"
   ],
   "id": "fd0a2275e91cc855",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.785722Z",
     "start_time": "2025-03-26T20:54:33.782595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stringify(obj):\n",
    "    \"\"\"\n",
    "    Converts a list or dictionary into a string representation.\n",
    "    - Uses JSON format with compact or pretty formatting based on the depth.\n",
    "    - Sorts keys in dictionaries for consistency.\n",
    "\n",
    "    :param obj: Any object (list, dict, or other Python object)\n",
    "    :return: String representation\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        return json.dumps(obj, sort_keys=True, ensure_ascii=False)  # Compact format\n",
    "    return str(obj)  # Fallback for other types"
   ],
   "id": "33c9fd9370a344aa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.835390Z",
     "start_time": "2025-03-26T20:54:33.830495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stringify_singleton_dict_list(dict_list):\n",
    "    \"\"\"\n",
    "    Processes a list of dictionaries:\n",
    "    - Removes the 'type' key from each dictionary.\n",
    "    - Tracks the largest dictionary by key count (excluding 'type').\n",
    "    - If the largest dictionary has only 1 key, extracts values, sorts them, and returns a pipe-concatenated string.\n",
    "    - Otherwise, returns an empty string.\n",
    "\n",
    "    :param dict_list: List of dictionaries to process\n",
    "    :return: Pipe-concatenated sorted values if all dicts have at most one key (excluding 'type'), else an empty string.\n",
    "    \"\"\"\n",
    "    if not isinstance(dict_list, list) or not all(isinstance(d, dict) for d in dict_list):\n",
    "        # raise ValueError(\"Input must be a list of dictionaries\")\n",
    "        return \"\"\n",
    "\n",
    "    largest_key_count = 0\n",
    "    processed_values = []\n",
    "\n",
    "    for d in dict_list:\n",
    "        cleaned_dict = {k: v for k, v in d.items() if k != \"type\"}  # Remove 'type'\n",
    "        largest_key_count = max(largest_key_count, len(cleaned_dict))\n",
    "        if len(cleaned_dict) == 1:  # If it only has one key, extract its value\n",
    "            processed_values.append(next(iter(cleaned_dict.values())))\n",
    "\n",
    "    # If the largest dictionary has only one key, return sorted pipe-concatenated string\n",
    "    if largest_key_count == 1:\n",
    "        return \"|\".join(map(str, sorted(processed_values)))  # Sort values before joining\n",
    "\n",
    "    return \"\"  # Return empty string if dicts contain more than one unique key\n"
   ],
   "id": "7b9727e83ab5009e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.883072Z",
     "start_time": "2025-03-26T20:54:33.878629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_all_documents(collection_name, page_size=10):\n",
    "    \"\"\"\n",
    "    Fetch all documents from a specified NMDC collection using paging.\n",
    "\n",
    "    :param collection_name: Name of the NMDC collection (e.g., \"study_set\", \"biosample_set\")\n",
    "    :param page_size: Number of documents per page (default: 99)\n",
    "    :return: List of documents from the collection\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    next_page_token = None\n",
    "    url = f\"{NMDC_RUNTIME_API_BASE_URL}{collection_name}\"\n",
    "\n",
    "    while True:\n",
    "        params = {\"max_page_size\": page_size}\n",
    "        if next_page_token:\n",
    "            print(f\"{next_page_token = }\")\n",
    "            params[\"page_token\"] = next_page_token\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        data = response.json()\n",
    "\n",
    "        documents.extend(data.get(\"resources\", []))  # Using \"resources\" as the key for results\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "\n",
    "        if not next_page_token:  # Stop if there's no more data to fetch\n",
    "            break\n",
    "\n",
    "    return documents"
   ],
   "id": "a250ed97f0ad4b9d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.946805Z",
     "start_time": "2025-03-26T20:54:33.934941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten(documents, ctv_slots=None, known_skips=None):\n",
    "    \"\"\"\n",
    "    Extracts scalar fields from documents, concatenating lists of scalars with a pipe ('|') separator.\n",
    "    If a field contains a dictionary with only scalar values, it is flattened using 'outer_key.inner_key' notation.\n",
    "    Lists of scalars within dictionaries are also pipe-concatenated.\n",
    "    Skips 'type' fields.\n",
    "\n",
    "    Special handling for ControlledIdentifiedTerm range slots like 'env_broad_scale', 'env_local_scale', and 'env_medium' etc.:\n",
    "      - Extracts 'has_raw_value', 'term.id', and 'term.name' if available.\n",
    "\n",
    "    Prints the field name if it encounters a complex structure (nested dicts, lists of dicts, etc.).\n",
    "\n",
    "    :param documents: List of documents (e.g., studies, biosamples)\n",
    "    :param ctv_slots: Set of slots whose range is a ControlledTermValue or descendant\n",
    "    :param known_skips: Set of slots that can't be stringified nicely\n",
    "    :return: List of dictionaries containing only scalar fields, with lists of scalars pipe-concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    if ctv_slots is None:\n",
    "        ctv_slots = set()\n",
    "\n",
    "    if known_skips is None:\n",
    "        known_skips = set()\n",
    "\n",
    "    scalar_docs = []\n",
    "    problem_slots = set()\n",
    "\n",
    "    for doc in documents:\n",
    "        scalar_doc = {}\n",
    "        stringified_singletons = \"\"  # Ensure fresh value per row\n",
    "\n",
    "        for key, value in doc.items():\n",
    "            if key == \"type\":\n",
    "                continue  # Skip 'type' fields\n",
    "\n",
    "            if key in known_skips:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, (str, int, float, bool, type(None))):\n",
    "                scalar_doc[key] = value  # Keep scalars as-is\n",
    "\n",
    "            elif isinstance(value, list) and all(\n",
    "                    isinstance(item, (str, int, float, bool, type(None))) for item in value):\n",
    "                scalar_doc[key] = \"|\".join(map(str, value))  # Join list elements with '|'\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                if key in ctv_slots:\n",
    "                    # Extract values from env_* fields\n",
    "                    if \"has_raw_value\" in value:\n",
    "                        scalar_doc[f\"{key}_has_raw_value\"] = value[\"has_raw_value\"]\n",
    "\n",
    "                    if \"term\" in value and isinstance(value[\"term\"], dict):\n",
    "                        if \"id\" in value[\"term\"]:\n",
    "                            scalar_doc[f\"{key}_term.id\"] = value[\"term\"][\"id\"]\n",
    "                        if \"name\" in value[\"term\"]:\n",
    "                            scalar_doc[f\"{key}_term.name\"] = value[\"term\"][\"name\"]\n",
    "\n",
    "                elif all(isinstance(v, (str, int, float, bool, type(None))) or\n",
    "                         (isinstance(v, list) and all(\n",
    "                             isinstance(item, (str, int, float, bool, type(None))) for item in v))\n",
    "                         for v in value.values()):\n",
    "                    # Flatten scalar-only dicts and pipe-join any scalar lists\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if sub_key == \"type\":\n",
    "                            continue  # Skip 'type' fields inside dicts\n",
    "                        if isinstance(sub_value, list):\n",
    "                            scalar_doc[f\"{key}_{sub_key}\"] = \"|\".join(map(str, sub_value))\n",
    "                        else:\n",
    "                            scalar_doc[f\"{key}_{sub_key}\"] = sub_value\n",
    "\n",
    "                else:\n",
    "                    # print(f\"Skipping structured field: {key}\")  # Print field name if dict has nested structures\n",
    "                    problem_slots.add(key)\n",
    "                    stringified_singletons = stringify_singleton_dict_list(value)\n",
    "                    if stringified_singletons != \"\":\n",
    "                        scalar_doc[f\"{key}\"] = stringified_singletons\n",
    "                    else:\n",
    "                        if key not in known_skips:\n",
    "                            scalar_doc[f\"{key}\"] = stringify(value)\n",
    "            else:\n",
    "                # print(f\"Skipping structured field: {key}\")  # Print field name for unhandled complex structures\n",
    "                problem_slots.add(key)\n",
    "                stringified_singletons = stringify_singleton_dict_list(value)\n",
    "                if stringified_singletons != \"\":\n",
    "                    scalar_doc[f\"{key}\"] = stringified_singletons\n",
    "                else:\n",
    "                    if key not in known_skips:\n",
    "                        scalar_doc[f\"{key}\"] = stringify(value)\n",
    "\n",
    "        scalar_docs.append(scalar_doc)\n",
    "\n",
    "    stringifieds = sorted(problem_slots - known_skips)  # Ensure it's sorted before printing\n",
    "\n",
    "    print(f\"stringified: {stringifieds}\")\n",
    "\n",
    "    return scalar_docs\n"
   ],
   "id": "da8a5b0027939908",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:33.986596Z",
     "start_time": "2025-03-26T20:54:33.982917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_associated_dois(studies):\n",
    "    \"\"\"\n",
    "    Extracts associated_dois from a list of studies, adding the study's ID to each DOI entry.\n",
    "\n",
    "    :param studies: List of study documents\n",
    "    :return: List of dictionaries, each representing a DOI with the study ID added\n",
    "    \"\"\"\n",
    "    doi_entries = []\n",
    "\n",
    "    for study in studies:\n",
    "        study_id = study.get(\"id\")  # Get study ID\n",
    "        associated_dois = study.get(\"associated_dois\", [])\n",
    "\n",
    "        for doi in associated_dois:\n",
    "            if isinstance(doi, dict):  # Ensure it's a dictionary\n",
    "                doi_entry = doi.copy()  # Make a copy to avoid modifying original data\n",
    "                doi_entry[\"study_id\"] = study_id  # Add study ID\n",
    "                doi_entry.pop(\"type\", None)  # Remove 'type' field if it exists\n",
    "                doi_entries.append(doi_entry)\n",
    "\n",
    "    return doi_entries\n"
   ],
   "id": "829ced5b2e9794d0",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.036523Z",
     "start_time": "2025-03-26T20:54:34.030976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_credit_associations(studies):\n",
    "    \"\"\"\n",
    "    Extracts credit associations from a list of studies, flattening PersonValue fields and pipe-concatenating applied roles.\n",
    "\n",
    "    :param studies: List of study documents\n",
    "    :return: List of dictionaries, each representing a CreditAssociation with the study ID added\n",
    "    \"\"\"\n",
    "    credit_entries = []\n",
    "\n",
    "    for study in studies:\n",
    "        study_id = study.get(\"id\")  # Get study ID\n",
    "        credit_associations = study.get(\"has_credit_associations\", [])\n",
    "\n",
    "        for credit in credit_associations:\n",
    "            if isinstance(credit, dict):  # Ensure it's a dictionary\n",
    "                credit_entry = {\"study_id\": study_id}  # Start with the study ID\n",
    "\n",
    "                # Flatten applied_roles (if multivalued)\n",
    "                applied_roles = credit.get(\"applied_roles\", [])\n",
    "                if isinstance(applied_roles, list):\n",
    "                    applied_roles.sort()\n",
    "                    credit_entry[\"applied_roles\"] = \"|\".join(map(str, applied_roles))\n",
    "\n",
    "                # Flatten applies_to_person (PersonValue structure)\n",
    "                person = credit.get(\"applies_to_person\", {})\n",
    "                if isinstance(person, dict):\n",
    "                    for key in [\"name\", \"orcid\", \"profile_image_url\", \"has_raw_value\"]:\n",
    "                        if key in person:\n",
    "                            credit_entry[f\"person.{key}\"] = person[key]\n",
    "\n",
    "                    # Pipe-concatenate websites if it's a list\n",
    "                    if isinstance(person.get(\"websites\"), list):\n",
    "                        credit_entry[\"person.websites\"] = \"|\".join(person[\"websites\"])\n",
    "\n",
    "                credit_entries.append(credit_entry)\n",
    "\n",
    "    return credit_entries\n"
   ],
   "id": "bfc90e9183205cdf",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.084767Z",
     "start_time": "2025-03-26T20:54:34.079198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_chem_administration(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts chem_administration field from biosample documents,\n",
    "    creating a separate table with biosample ID, chemical name, chemical ID,\n",
    "    raw value, and timestamp.\n",
    "\n",
    "    :param biosamples: List of biosample documents\n",
    "    :return: List of dictionaries, each representing a chemical administration entry\n",
    "    \"\"\"\n",
    "    chem_entries = []\n",
    "    pattern = re.compile(r\"^(.*?) \\[([^\\]]+)\\];([\\d\\-T:]+)$\")  # Regex for label, CURIE, timestamp\n",
    "\n",
    "    for sample in biosamples:\n",
    "        biosample_id = sample.get(\"id\")  # Get biosample ID\n",
    "        chem_administration = sample.get(\"chem_administration\", [])\n",
    "\n",
    "        for chem in chem_administration:\n",
    "            if isinstance(chem, dict):\n",
    "                entry = {\"biosample_id\": biosample_id}\n",
    "\n",
    "                # Extract has_raw_value and parse it\n",
    "                raw_value = chem.get(\"has_raw_value\", \"\")\n",
    "                entry[\"has_raw_value\"] = raw_value\n",
    "\n",
    "                match = pattern.match(raw_value)\n",
    "                if match:\n",
    "                    entry[\"extracted_label\"] = match.group(1)\n",
    "                    entry[\"extracted_curie\"] = match.group(2)\n",
    "                    entry[\"extracted_timestamp\"] = match.group(3)\n",
    "                else:\n",
    "                    entry[\"extracted_label\"] = \"\"\n",
    "                    entry[\"extracted_curie\"] = \"\"\n",
    "                    entry[\"extracted_timestamp\"] = \"\"\n",
    "\n",
    "                # Extract term details if present\n",
    "                term = chem.get(\"term\", {})\n",
    "                if isinstance(term, dict):\n",
    "                    entry[\"term_id\"] = term.get(\"id\", \"\")\n",
    "                    entry[\"term_name\"] = term.get(\"name\", \"\")\n",
    "\n",
    "                chem_entries.append(entry)\n",
    "\n",
    "    return chem_entries\n"
   ],
   "id": "75e55f9e5ddb55dd",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.129047Z",
     "start_time": "2025-03-26T20:54:34.124597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_gold_contacts(records, idcol):\n",
    "    \"\"\"\n",
    "    Extracts contacts from biosample documents, creating a structured table with\n",
    "    biosample ID, contact name, email, jgiSsoId, and roles.\n",
    "\n",
    "    :param biosamples: List of biosample documents\n",
    "    :return: List of dictionaries, each representing a contact with the biosample ID\n",
    "    \"\"\"\n",
    "    contact_entries = []\n",
    "\n",
    "    for sample in records:\n",
    "        record_id = sample.get(idcol)  # Get biosample ID\n",
    "        contacts = sample.get(\"contacts\", [])\n",
    "\n",
    "        for contact in contacts:\n",
    "            if isinstance(contact, dict):\n",
    "                entry = {\n",
    "                    \"id\": record_id,\n",
    "                    \"name\": contact.get(\"name\", \"\"),\n",
    "                    \"email\": contact.get(\"email\", \"\"),\n",
    "                    \"jgiSsoId\": contact.get(\"jgiSsoId\", \"\"),\n",
    "                    \"roles\": \"|\".join(sorted(contact.get(\"roles\", [])))  # Pipe-concatenate sorted roles\n",
    "                }\n",
    "                contact_entries.append(entry)\n",
    "\n",
    "    return contact_entries\n"
   ],
   "id": "2a644bd63244b2c5",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.176011Z",
     "start_time": "2025-03-26T20:54:34.171831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_harmonized_attributes(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts attributes from biosample documents, keeping only those with a 'harmonized_name' key.\n",
    "    The result is a structured table where each biosample is represented by its accession and\n",
    "    selected attributes mapped by harmonized_name.\n",
    "\n",
    "    :param biosamples: List of biosample documents\n",
    "    :return: List of dictionaries, each representing a biosample with selected attributes\n",
    "    \"\"\"\n",
    "    extracted_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        biosample_entry = {\"accession\": sample.get(\"accession\", \"\")}  # Retain accession\n",
    "\n",
    "        # Extract attributes list\n",
    "        attributes_list = sample.get(\"Attributes\", {}).get(\"Attribute\", [])  # Use {} to avoid KeyError\n",
    "\n",
    "        for attribute in attributes_list:\n",
    "            if isinstance(attribute, dict):  # Ensure attribute is a dictionary\n",
    "                harmonized_name = attribute.get(\"harmonized_name\")\n",
    "                content = attribute.get(\"content\", \"\")\n",
    "\n",
    "                if harmonized_name:  # Retain only attributes with harmonized_name\n",
    "                    biosample_entry[harmonized_name] = content\n",
    "\n",
    "        extracted_entries.append(biosample_entry)\n",
    "\n",
    "    return extracted_entries\n"
   ],
   "id": "1dd532af511686d2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.223838Z",
     "start_time": "2025-03-26T20:54:34.219832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_all_attributes(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts all attributes from biosample documents and appends the biosample's 'accession' field.\n",
    "    Each attribute is stored as a separate dictionary entry.\n",
    "\n",
    "    :param biosamples: List of biosample dictionaries\n",
    "    :return: List of dictionaries, each representing an attribute with its associated biosample accession\n",
    "    \"\"\"\n",
    "    extracted_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        accession = sample.get(\"accession\", \"\")  # Retain accession\n",
    "\n",
    "        # Extract attributes list\n",
    "        attributes_list = sample.get(\"Attributes\", {}).get(\"Attribute\", [])  # Use {} to avoid KeyError\n",
    "\n",
    "        for attribute in attributes_list:\n",
    "            if isinstance(attribute, dict):  # Ensure attribute is a dictionary\n",
    "                # Copy attribute and add accession field\n",
    "                attribute_entry = attribute.copy()\n",
    "                attribute_entry[\"accession\"] = accession\n",
    "                extracted_entries.append(attribute_entry)\n",
    "\n",
    "    return extracted_entries\n"
   ],
   "id": "b6cd83e6ec27ae2d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.270845Z",
     "start_time": "2025-03-26T20:54:34.266682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_biosample_ids(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts IDs from biosample documents and appends the biosample's 'accession' field.\n",
    "    Each ID entry is stored as a separate dictionary.\n",
    "\n",
    "    :param biosamples: List of biosample dictionaries\n",
    "    :return: List of dictionaries, each representing an ID with its associated biosample accession\n",
    "    \"\"\"\n",
    "    extracted_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        accession = sample.get(\"accession\", \"\")  # Retain accession\n",
    "\n",
    "        # Extract Ids list (assuming it's a dictionary stored as a JSON-like structure)\n",
    "        ids_data = sample.get(\"Ids\", {})\n",
    "\n",
    "        # Ensure it's a dictionary and extract the list of Ids\n",
    "        ids_list = ids_data.get(\"Id\", [])\n",
    "\n",
    "        if not isinstance(ids_list, list):\n",
    "            continue  # Skip if Ids is not a list (avoids errors)\n",
    "\n",
    "        for id_entry in ids_list:\n",
    "            if isinstance(id_entry, dict):  # Ensure ID entry is a dictionary\n",
    "                # Copy ID entry and add accession field\n",
    "                id_entry_copy = id_entry.copy()\n",
    "                id_entry_copy[\"accession\"] = accession\n",
    "                extracted_entries.append(id_entry_copy)\n",
    "\n",
    "    return extracted_entries\n"
   ],
   "id": "f5f8e75e03373307",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.318484Z",
     "start_time": "2025-03-26T20:54:34.313840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_biosample_models_content(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts X from biosample documents and appends the biosample's 'accession' field.\n",
    "    Each X is stored as a separate dictionary.\n",
    "\n",
    "    :param biosamples: List of biosample dictionaries\n",
    "    :return: List of dictionaries, each representing an X with its associated biosample accession\n",
    "    \"\"\"\n",
    "\n",
    "    # todo ignores the moderate possibility that non-EMP500 studies might user other model sub-fields like version\n",
    "    extracted_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        accession = sample.get(\"accession\", \"\")  # Retain accession\n",
    "\n",
    "        # Extract Ids list (assuming it's a dictionary stored as a JSON-like structure)\n",
    "        x_data = sample.get(\"Models\", {})\n",
    "\n",
    "        # is this the kind of thing that could be a dict or a list?\n",
    "        x_inner = x_data.get(\"Model\", [])\n",
    "\n",
    "        if isinstance(x_inner, dict):\n",
    "            x_inner[\"accession\"] = accession\n",
    "            extracted_entries.append(x_inner)\n",
    "\n",
    "        if isinstance(x_inner, dict):\n",
    "            for x_entry in x_inner:\n",
    "                if isinstance(x_entry, dict):  # Ensure ID entry is a dictionary\n",
    "                    # Copy ID entry and add accession field\n",
    "                    x_entry_copy = x_entry.copy()\n",
    "                    x_entry_copy[\"accession\"] = accession\n",
    "                    extracted_entries.append(x_entry_copy)\n",
    "\n",
    "    return extracted_entries\n"
   ],
   "id": "72989862a4a78bc6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.365477Z",
     "start_time": "2025-03-26T20:54:34.361795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_biosample_owner_name_content(biosamples):\n",
    "    \"\"\"\n",
    "    Extracts X from biosample documents and appends the biosample's 'accession' field.\n",
    "    Each X is stored as a separate dictionary.\n",
    "\n",
    "    :param biosamples: List of biosample dictionaries\n",
    "    :return: List of dictionaries, each representing an X with its associated biosample accession\n",
    "    \"\"\"\n",
    "\n",
    "    # todo ignores the strong possibility that non-EMP500 studies might some of the many other owner sub-fields\n",
    "\n",
    "    extracted_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        accession = sample.get(\"accession\", \"\")  # Retain accession\n",
    "\n",
    "        owner = sample.get(\"Owner\", [])\n",
    "        owner_name = owner.get(\"Name\", [])\n",
    "        owner_name['accession'] = accession\n",
    "        extracted_entries.append(owner_name)\n",
    "\n",
    "    return extracted_entries\n"
   ],
   "id": "2eb9a9fb3bbe9ff2",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.413759Z",
     "start_time": "2025-03-26T20:54:34.408981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_ncbi_emp500_biosample_descriptions(biosamples):\n",
    "    \"\"\"\n",
    "    Flattens biosample descriptions into a structured format, extracting:\n",
    "    - Accession (from Title)\n",
    "    - Taxonomy ID and Name (from Organism)\n",
    "    - Organism Name (from Organism.OrganismName.content)\n",
    "    - Description (from Comment.Paragraph.content)\n",
    "\n",
    "    :param biosamples: List of biosample dictionaries\n",
    "    :return: List of flattened biosample records\n",
    "    \"\"\"\n",
    "    flattened_entries = []\n",
    "\n",
    "    for sample in biosamples:\n",
    "        flattened_entry = {}\n",
    "\n",
    "        accession = sample.get(\"accession\", \"\")\n",
    "\n",
    "        biosample_desc = sample.get(\"Description\", {})\n",
    "\n",
    "        # Extract accession from Title\n",
    "        flattened_entry[\"accession\"] = accession\n",
    "\n",
    "        # Extract taxonomy information from Organism\n",
    "        organism = biosample_desc.get(\"Organism\", {})\n",
    "        flattened_entry[\"taxonomy_id\"] = organism.get(\"taxonomy_id\", \"\")\n",
    "        flattened_entry[\"taxonomy_name\"] = organism.get(\"taxonomy_name\", \"\")\n",
    "        flattened_entry[\"organism_name\"] = organism.get(\"OrganismName\", {}).get(\"content\", \"\")\n",
    "\n",
    "        # Extract sample description from Comment\n",
    "        comment = biosample_desc.get(\"Comment\", {}).get(\"Paragraph\", {})\n",
    "        flattened_entry[\"description\"] = comment.get(\"content\", \"\")\n",
    "\n",
    "        flattened_entries.append(flattened_entry)\n",
    "\n",
    "    return flattened_entries\n"
   ],
   "id": "c1976ad9831348c4",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.461904Z",
     "start_time": "2025-03-26T20:54:34.457793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reorder_columns(df, primary_columns, do_sort=True):\n",
    "    \"\"\"\n",
    "    Reorders a DataFrame's columns so that specified columns appear first in the given order,\n",
    "    followed by all other columns sorted alphabetically.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param primary_columns: List of columns to move to the first positions in order\n",
    "    :return: DataFrame with reordered columns\n",
    "    \"\"\"\n",
    "    if not isinstance(primary_columns, list):\n",
    "        raise ValueError(\"primary_columns must be a list\")\n",
    "\n",
    "    missing_columns = [col for col in primary_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Columns {missing_columns} not found in DataFrame\")\n",
    "\n",
    "    remaining_columns = [col for col in df.columns if col not in primary_columns]  # Exclude primary columns\n",
    "    if do_sort:\n",
    "        remaining_columns = sorted(remaining_columns)  # Sort remaining columns alphabetically\n",
    "    new_order = primary_columns + remaining_columns  # Place primary columns first in order\n",
    "    return df[new_order]  # Reorder DataFrame\n"
   ],
   "id": "b5a88afdf5d3064f",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.510997Z",
     "start_time": "2025-03-26T20:54:34.508293Z"
    }
   },
   "cell_type": "code",
   "source": "ever_seen = set() # all values found when checking to_label_check slots",
   "id": "59e42d67ced1984f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:34.582781Z",
     "start_time": "2025-03-26T20:54:34.554346Z"
    }
   },
   "cell_type": "code",
   "source": "ontology_adapters = build_ontology_adapters(ontology_list)",
   "id": "8ba934144a44fafb",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:49.648150Z",
     "start_time": "2025-03-26T20:54:34.604231Z"
    }
   },
   "cell_type": "code",
   "source": "label_cache = load_ontology_labels(ontology_adapters)",
   "id": "50f9675db16dd349",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:49.682612Z",
     "start_time": "2025-03-26T20:54:49.657292Z"
    }
   },
   "cell_type": "code",
   "source": "obsolete_terms_list = find_obsolete_terms(ontology_adapters)",
   "id": "cb359407173a9411",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:49.709372Z",
     "start_time": "2025-03-26T20:54:49.707094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# determine what nmdc-schema slots on which classes might contain links to external biosamples, studies or article DOIs\n",
    "\n",
    "# anything outside of https://microbiomedata.github.io/nmdc-schema/alternative_identifiers/ ->\n",
    "#   https://microbiomedata.github.io/nmdc-schema/external_database_identifiers/ ?"
   ],
   "id": "f9f274dbff84edab",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:51.122801Z",
     "start_time": "2025-03-26T20:54:49.749686Z"
    }
   },
   "cell_type": "code",
   "source": "nmdc_schema_view = SchemaView(nmdc_schema_url)",
   "id": "2e8a0ed0a0e78d93",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:52.273840Z",
     "start_time": "2025-03-26T20:54:52.258356Z"
    }
   },
   "cell_type": "code",
   "source": "alternative_identifier_descendants = nmdc_schema_view.slot_descendants(\"alternative_identifiers\")",
   "id": "17e0b6f25cc50a20",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:54.531949Z",
     "start_time": "2025-03-26T20:54:54.529344Z"
    }
   },
   "cell_type": "code",
   "source": "alternative_identifier_descendants.sort()",
   "id": "4a05b58f572a7a3e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:54.554339Z",
     "start_time": "2025-03-26T20:54:54.535280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aid_rows = []\n",
    "for aid_name in alternative_identifier_descendants:\n",
    "    aid = nmdc_schema_view.get_slot(aid_name)\n",
    "    aid_mixins = aid['mixins']\n",
    "    aid_is_a = aid.is_a\n",
    "    aid_classes = nmdc_schema_view.get_classes_by_slot(aid)\n",
    "    aid_class_descendant_names = set()\n",
    "    for c in aid_classes:\n",
    "        c_descendants = nmdc_schema_view.class_descendants(c)\n",
    "        aid_class_descendant_names.update(c_descendants)\n",
    "    aid_class_descendant_names = list(aid_class_descendant_names)\n",
    "    aid_class_descendant_names.sort()\n",
    "    aid_rows.append({\n",
    "        'slot': aid_name,\n",
    "        'parent': aid_is_a,\n",
    "        'mixins': aid_mixins,\n",
    "        'classes_using': aid_class_descendant_names,\n",
    "    })"
   ],
   "id": "a151e049f60c5082",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:55.735421Z",
     "start_time": "2025-03-26T20:54:55.732534Z"
    }
   },
   "cell_type": "code",
   "source": "aid_frame = pd.DataFrame(aid_rows)",
   "id": "9dc1afc53d037747",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:57.993085Z",
     "start_time": "2025-03-26T20:54:57.988646Z"
    }
   },
   "cell_type": "code",
   "source": "aid_frame.to_csv(aid_tsv, sep=\"\\t\", index=False)",
   "id": "b84b963b14264945",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "7dfb4086d8b5bf22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:58.841806Z",
     "start_time": "2025-03-26T20:54:57.998793Z"
    }
   },
   "cell_type": "code",
   "source": "nmdc_schema_usage_index = nmdc_schema_view.usage_index() # collections.defaultdict; Dict[ElementName, List[SchemaUsage]]",
   "id": "34dfc3ff1390dfe8",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:54:59.999808Z",
     "start_time": "2025-03-26T20:54:59.997373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctv_usage = nmdc_schema_usage_index['ControlledTermValue'] # list of linkml_runtime.utils.schemaview.SchemaUsage\n",
    "\n",
    "# SchemaUsage(used_by='Biosample', slot='chem_administration', metaslot='range', used='ControlledTermValue', inferred=True)"
   ],
   "id": "47f4c62d606302a6",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:55:02.348378Z",
     "start_time": "2025-03-26T20:55:02.345781Z"
    }
   },
   "cell_type": "code",
   "source": "citv_usage = nmdc_schema_usage_index['ControlledIdentifiedTermValue']",
   "id": "83c0f24cc928346e",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:55:03.498018Z",
     "start_time": "2025-03-26T20:55:03.494778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctv_using_slots = set()\n",
    "for i in ctv_usage:\n",
    "    ctv_using_slots.add(i.slot)\n",
    "print(len(ctv_using_slots))"
   ],
   "id": "b47fa38e6c9f3608",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:55:03.535375Z",
     "start_time": "2025-03-26T20:55:03.504007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in citv_usage:\n",
    "    ctv_using_slots.add(i.slot)\n",
    "print(len(ctv_using_slots))"
   ],
   "id": "a7e1410fc69bd6de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:55:04.851651Z",
     "start_time": "2025-03-26T20:55:04.845775Z"
    }
   },
   "cell_type": "code",
   "source": "ctv_using_slots",
   "id": "ca3aee5e6d3ce6ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chem_administration',\n",
       " 'env_broad_scale',\n",
       " 'env_local_scale',\n",
       " 'env_medium',\n",
       " 'experimental_factor',\n",
       " 'feature_category',\n",
       " 'growth_facil',\n",
       " 'host_body_product',\n",
       " 'host_body_site',\n",
       " 'host_phenotype',\n",
       " 'host_taxid',\n",
       " 'plant_growth_med',\n",
       " 'plant_struc',\n",
       " 'samp_mat_process',\n",
       " 'samp_taxon_id'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "a2d6a1e4850b4b01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
