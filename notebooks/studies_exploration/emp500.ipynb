{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.469295Z",
     "start_time": "2025-01-28T20:05:11.352544Z"
    }
   },
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "import csv\n",
    "\n",
    "from oaklib import get_adapter\n",
    "\n",
    "from oaklib.datamodels.vocabulary import IS_A"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.477654Z",
     "start_time": "2025-01-28T20:05:14.473328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connection_string = \"mongodb://localhost:27017/\"\n",
    "\n",
    "envo_adapter_string = 'sqlite:obo:envo'\n",
    "\n",
    "gold_study_id = \"Gs0154244\"\n",
    "\n",
    "# no bioproject identifier is  included in the NCBI EMP500 biosample records\n",
    "# but they might all have emp500_principal_investigator attributes\n",
    "# indexing on Attributes.Attribute.attribute_name took ~ 30 minutes. resulting index size is ~ 3 GB\n",
    "# may need to index MongoDB on that\n",
    "# could alternatively use the DuckDB\n",
    "ncbi_project_id = \"17119329\"\n",
    "ncbi_project_accession = \"PRJEB42019\"\n",
    "ncbi_emp500_evidence_attribute = \"emp500_principal_investigator\"\n",
    "nmdc_study_id = \"nmdc:sty-11-547rwq94\"\n",
    "\n",
    "ncbi_output_tsv = \"ncbi_emp500_environmental_triads.tsv\"\n",
    "\n",
    "gold_seq_proj_output_tsv = \"gold_emp500_seq_proj_links.tsv\"\n",
    "\n",
    "gold_biosamples_output_tsv = \"gold_emp500_environments_ecosystems.tsv\"\n",
    "\n",
    "merged_flagged_output_tsv = \"merged_emp500_data.tsv\""
   ],
   "id": "8e022237dde79cf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.544488Z",
     "start_time": "2025-01-28T20:05:14.537024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to MongoDB server running on localhost\n",
    "client = MongoClient(connection_string)"
   ],
   "id": "31eb01385603b3f5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.590008Z",
     "start_time": "2025-01-28T20:05:14.587184Z"
    }
   },
   "cell_type": "code",
   "source": "gold_db = client[\"gold_metadata\"]",
   "id": "f3fb4cf981fa622b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.632810Z",
     "start_time": "2025-01-28T20:05:14.629329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ncbi_db = client[\"biosamples\"]\n",
    "ncbi_biosamples_collection = ncbi_db[\"biosamples\"]"
   ],
   "id": "d1667d0d4ca07f14",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.680153Z",
     "start_time": "2025-01-28T20:05:14.676445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the query\n",
    "emp500_biosamples_query = {\"Attributes.Attribute.attribute_name\": ncbi_emp500_evidence_attribute}"
   ],
   "id": "901b7e10e7e23086",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.795062Z",
     "start_time": "2025-01-28T20:05:14.723777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fetch matching records and convert them to a list of dictionaries\n",
    "ncbi_emp500_biosamples = list(ncbi_biosamples_collection.find(emp500_biosamples_query))\n"
   ],
   "id": "b7ce2b691fb780c3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.807259Z",
     "start_time": "2025-01-28T20:05:14.804245Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(ncbi_emp500_biosamples))",
   "id": "6313bfe7d44acc7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.870965Z",
     "start_time": "2025-01-28T20:05:14.849055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the harmonized names you're looking for\n",
    "target_harmonized_names = {'env_broad_scale', 'env_local_scale', 'env_medium'}\n",
    "\n",
    "# Initialize an empty list to store the result\n",
    "result_list = []\n",
    "\n",
    "# Iterate through the matching records\n",
    "for record in ncbi_emp500_biosamples:\n",
    "    # Extract the accession and ncbi_project_accession values\n",
    "    accession = record.get('accession')\n",
    "    ncbi_bioproject_accession = record.get('ncbi_project_accession')\n",
    "\n",
    "    # Extract the values for the target paths\n",
    "    attribute_values = {}\n",
    "    attributes = record.get('Attributes', {}).get('Attribute', [])\n",
    "    for attribute in attributes:\n",
    "        harmonized_name = attribute.get('harmonized_name')\n",
    "        if harmonized_name in target_harmonized_names:\n",
    "            attribute_values[harmonized_name] = attribute.get('content')\n",
    "\n",
    "    # Add the extracted values to the result list\n",
    "    result_list.append({\n",
    "        'ncbi_bioproject_accession': ncbi_project_accession,\n",
    "        'ncbi_biosample_accession': accession,\n",
    "        **attribute_values  # Unpack the extracted attributes\n",
    "    })\n"
   ],
   "id": "8dc01a6983835c29",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:14.929325Z",
     "start_time": "2025-01-28T20:05:14.915514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the header for the TSV file\n",
    "header = ['ncbi_bioproject_accession', 'ncbi_biosample_accession', 'env_broad_scale', 'env_local_scale', 'env_medium']\n",
    "\n",
    "# Write the result list to the TSV file\n",
    "with open(ncbi_output_tsv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header, delimiter='\\t')\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    for item in result_list:\n",
    "        writer.writerow(item)\n",
    "\n",
    "print(f\"Data has been written to {ncbi_output_tsv}\")"
   ],
   "id": "be02b3938b48b607",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ncbi_emp500_environmental_triads.tsv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.022791Z",
     "start_time": "2025-01-28T20:05:14.966837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the query\n",
    "gold_emp500_seq_projs_query = {\"ncbiBioProjectAccession\": \"PRJEB42019\"}\n",
    "\n",
    "gold_seq_projs_collection = gold_db[\"projects\"]\n",
    "\n",
    "# Fetch matching records\n",
    "gold_emp500_seq_projs = list(gold_seq_projs_collection.find(gold_emp500_seq_projs_query))\n",
    "\n",
    "# Initialize a list to store the processed data\n",
    "result_table = []\n",
    "\n",
    "# Process each matching record\n",
    "for record in gold_emp500_seq_projs:\n",
    "    # Extract required fields with default values for missing fields\n",
    "    project_gold_id = record.get('projectGoldId', '')\n",
    "    sequencingStrategy = record.get('sequencingStrategy', '')\n",
    "    projectStatus = record.get('projectStatus', '')\n",
    "    study_gold_id = record.get('studyGoldId', '')\n",
    "    biosample_gold_id = record.get('biosampleGoldId', '')\n",
    "    organism_gold_id = record.get('organismGoldId', '')\n",
    "    ncbi_bioproject_accession = record.get('ncbiBioProjectAccession', '')\n",
    "    ncbi_biosample_accession = record.get('ncbiBioSampleAccession', '')\n",
    "    sra_experiment_ids = record.get('sraExperimentIds', [])\n",
    "\n",
    "    # Pipe-concatenate the values in sraExperimentIds\n",
    "    sra_experiment_ids_str = '|'.join(sra_experiment_ids)\n",
    "\n",
    "    # Append the processed data as a dictionary\n",
    "    result_table.append({\n",
    "        'projectGoldId': project_gold_id,\n",
    "        'sequencingStrategy': sequencingStrategy,\n",
    "        'projectStatus': projectStatus,\n",
    "        'studyGoldId': study_gold_id,\n",
    "        'biosampleGoldId': biosample_gold_id,\n",
    "        'ncbiBioProjectAccession': ncbi_bioproject_accession,\n",
    "        'ncbiBioSampleAccession': ncbi_biosample_accession,\n",
    "        'sraExperimentIds': sra_experiment_ids_str\n",
    "    })\n",
    "\n",
    "header = ['projectGoldId', 'sequencingStrategy', 'projectStatus', 'studyGoldId', 'biosampleGoldId',\n",
    "          'ncbiBioProjectAccession', 'ncbiBioSampleAccession', 'sraExperimentIds']\n",
    "\n",
    "with open(gold_seq_proj_output_tsv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header, delimiter='\\t')\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    writer.writerows(result_table)\n",
    "\n",
    "print(f\"Data has been written to {gold_seq_proj_output_tsv}\")\n"
   ],
   "id": "bd2555bd00e4e3f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to gold_emp500_seq_proj_links.tsv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.042913Z",
     "start_time": "2025-01-28T20:05:15.038988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_biosample_gold_ids = list({record['biosampleGoldId'] for record in result_table if 'biosampleGoldId' in record})\n",
    "print(len(unique_biosample_gold_ids))"
   ],
   "id": "1b36c782a8d184a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.115127Z",
     "start_time": "2025-01-28T20:05:15.081470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query MongoDB for all documents where biosampleGoldId matches any value in unique_biosample_gold_ids\n",
    "query = {\"biosampleGoldId\": {\"$in\": unique_biosample_gold_ids}}\n",
    "\n",
    "gold_biosamples_collection = gold_db[\"biosamples\"]\n",
    "# Fetch matching documents and convert them to a list of dictionaries\n",
    "gold_emp500_biosamples = list(gold_biosamples_collection.find(query))\n",
    "\n",
    "# Print the number of matching documents and optionally inspect some of them\n",
    "print(f\"Found {len(gold_emp500_biosamples)} matching documents.\")\n",
    "\n",
    "# for doc in gold_emp500_biosamples[:5]:  # Print the first 5 documents for inspection\n",
    "#     pprint.pprint(doc)\n"
   ],
   "id": "9966c187964e85e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1024 matching documents.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.158063Z",
     "start_time": "2025-01-28T20:05:15.132339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize an empty list to store the processed data\n",
    "processed_data = []\n",
    "\n",
    "# Iterate through the documents in gold_emp500_biosamples\n",
    "for record in gold_emp500_biosamples:\n",
    "    # Extract the fields, using `.get()` for defensive handling of nulls\n",
    "    biosample_gold_id = record.get('biosampleGoldId', '')\n",
    "    ecosystem_path_id = record.get('ecosystemPathId', '')\n",
    "    ecosystem = record.get('ecosystem', '')\n",
    "    ecosystem_category = record.get('ecosystemCategory', '')\n",
    "    ecosystem_type = record.get('ecosystemType', '')\n",
    "    ecosystem_subtype = record.get('ecosystemSubtype', '')\n",
    "    specific_ecosystem = record.get('specificEcosystem', '')\n",
    "\n",
    "    # Extract and transform envo fields (replacing underscores with colons)\n",
    "    envo_broad_scale = record.get('envoBroadScale', {})\n",
    "    envo_broad_scale_id = envo_broad_scale.get('id', '').replace('_', ':')\n",
    "    envo_broad_scale_label = envo_broad_scale.get('label', '')\n",
    "\n",
    "    envo_local_scale = record.get('envoLocalScale', {})\n",
    "    envo_local_scale_id = envo_local_scale.get('id', '').replace('_', ':')\n",
    "    envo_local_scale_label = envo_local_scale.get('label', '')\n",
    "\n",
    "    envo_medium = record.get('envoMedium', {})\n",
    "    envo_medium_id = envo_medium.get('id', '').replace('_', ':')\n",
    "    envo_medium_label = envo_medium.get('label', '')\n",
    "\n",
    "    # Add the processed record to the list\n",
    "    processed_data.append({\n",
    "        'biosampleGoldId': biosample_gold_id,\n",
    "        'ecosystemPathId': ecosystem_path_id,\n",
    "        'ecosystem': ecosystem,\n",
    "        'ecosystemCategory': ecosystem_category,\n",
    "        'ecosystemType': ecosystem_type,\n",
    "        'ecosystemSubtype': ecosystem_subtype,\n",
    "        'specificEcosystem': specific_ecosystem,\n",
    "        'envoBroadScale.id': envo_broad_scale_id,\n",
    "        'envoBroadScale.label': envo_broad_scale_label,\n",
    "        'envoLocalScale.id': envo_local_scale_id,\n",
    "        'envoLocalScale.label': envo_local_scale_label,\n",
    "        'envoMedium.id': envo_medium_id,\n",
    "        'envoMedium.label': envo_medium_label,\n",
    "    })\n",
    "\n",
    "# Define the header for the TSV file\n",
    "header = [\n",
    "    'biosampleGoldId',\n",
    "    'ecosystemPathId',\n",
    "    'ecosystem',\n",
    "    'ecosystemCategory',\n",
    "    'ecosystemType',\n",
    "    'ecosystemSubtype',\n",
    "    'specificEcosystem',\n",
    "    'envoBroadScale.id',\n",
    "    'envoBroadScale.label',\n",
    "    'envoLocalScale.id',\n",
    "    'envoLocalScale.label',\n",
    "    'envoMedium.id',\n",
    "    'envoMedium.label',\n",
    "]\n",
    "\n",
    "# Write the processed data to a TSV file\n",
    "with open(gold_biosamples_output_tsv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header, delimiter='\\t')\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    writer.writerows(processed_data)\n",
    "\n",
    "print(f\"Processed data has been written to {gold_biosamples_output_tsv}\")\n"
   ],
   "id": "3eb0687ef698ddfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been written to gold_emp500_environments_ecosystems.tsv\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.198608Z",
     "start_time": "2025-01-28T20:05:15.194349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Close the connection\n",
    "client.close()"
   ],
   "id": "41fce517c75a320e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.261622Z",
     "start_time": "2025-01-28T20:05:15.240750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "filenames = [\n",
    "    \"gold_emp500_environments_ecosystems.tsv\",\n",
    "    \"gold_emp500_seq_proj_links.tsv\",\n",
    "    \"ncbi_emp500_environmental_triads.tsv\"\n",
    "]\n",
    "\n",
    "# Read each file into a DataFrame and assign it to a variable named after the base of the filename\n",
    "for filename in filenames:\n",
    "    # Extract the base name (without extension) and replace non-alphanumeric characters with underscores\n",
    "    var_name = filename.split('.')[0]\n",
    "    # Read the TSV into a DataFrame\n",
    "    globals()[var_name] = pd.read_csv(filename, sep='\\t', dtype=str)\n",
    "\n",
    "# Verify the loaded DataFrames\n",
    "print(\"Loaded DataFrames:\")\n",
    "for var_name in filenames:\n",
    "    var_name = var_name.split('.')[0]\n",
    "    print(f\"{var_name}: {globals()[var_name].shape}\")\n"
   ],
   "id": "66899a7062c85619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrames:\n",
      "gold_emp500_environments_ecosystems: (1024, 13)\n",
      "gold_emp500_seq_proj_links: (1836, 8)\n",
      "ncbi_emp500_environmental_triads: (1024, 5)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.308084Z",
     "start_time": "2025-01-28T20:05:15.292781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_merged_df = pd.merge(\n",
    "    gold_emp500_seq_proj_links,\n",
    "    ncbi_emp500_environmental_triads,\n",
    "    left_on=\"ncbiBioSampleAccession\",\n",
    "    right_on=\"ncbi_biosample_accession\",\n",
    "    how=\"outer\",\n",
    "    indicator=False  # Add an indicator column to track unmatched rows\n",
    ")\n",
    "\n",
    "# Join the resulting DataFrame with gold_emp500_environments_ecosystems\n",
    "merged_df = pd.merge(\n",
    "    pre_merged_df,\n",
    "    gold_emp500_environments_ecosystems,\n",
    "    left_on=\"biosampleGoldId\",\n",
    "    right_on=\"biosampleGoldId\",\n",
    "    how=\"outer\",\n",
    "    indicator=False  # Add an indicator column to track unmatched rows\n",
    ")"
   ],
   "id": "193068b040bc6bba",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.355515Z",
     "start_time": "2025-01-28T20:05:15.343393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Drop the specified columns\n",
    "columns_to_drop = ['projectGoldId', 'sequencingStrategy', 'projectStatus', 'sraExperimentIds']\n",
    "deduped_df = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Remove duplicate rows\n",
    "deduped_df = deduped_df.drop_duplicates()\n",
    "\n",
    "# Verify the resulting DataFrame\n",
    "print(f\"Final cleaned DataFrame shape (after dropping columns and removing duplicates): {deduped_df.shape}\")\n"
   ],
   "id": "dc8a60c208902a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned DataFrame shape (after dropping columns and removing duplicates): (1024, 21)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.428632Z",
     "start_time": "2025-01-28T20:05:15.395084Z"
    }
   },
   "cell_type": "code",
   "source": "envo_adapter = get_adapter(envo_adapter_string)",
   "id": "5d8255fb640963c9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.563008Z",
     "start_time": "2025-01-28T20:05:15.467236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the IDs for the classes\n",
    "biome_id = \"ENVO:00000428\"  # Biome\n",
    "environmental_material_id = \"ENVO:00010483\"  # Environmental material\n",
    "\n",
    "# Get all subclasses (descendants) of 'biome'\n",
    "biome_subclasses = envo_adapter.descendants(biome_id, reflexive=False, predicates=[IS_A])\n",
    "biome_subclasses = list(biome_subclasses)  # Convert to a list for easier handling\n",
    "\n",
    "# Get all subclasses (descendants) of 'environmental material'\n",
    "environmental_material_subclasses = envo_adapter.descendants(environmental_material_id, reflexive=False, predicates=[IS_A])\n",
    "environmental_material_subclasses = list(environmental_material_subclasses)  # Convert to a list\n"
   ],
   "id": "e8c195d5849fef43",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.604837Z",
     "start_time": "2025-01-28T20:05:15.600187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add 'bad_ebs' column: True if envoBroadScale.id is not in biome_subclasses\n",
    "deduped_df['bad_ebs'] = ~deduped_df['envoBroadScale.id'].isin(biome_subclasses)\n",
    "\n",
    "# Add 'bad_em' column: True if envoMedium.id is not in environmental_material_subclasses\n",
    "deduped_df['bad_em'] = ~deduped_df['envoMedium.id'].isin(environmental_material_subclasses)\n"
   ],
   "id": "7a6511f22281e376",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.770071Z",
     "start_time": "2025-01-28T20:05:15.687852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import textdistance\n",
    "\n",
    "# Function to compute cosine-like distances for pairs of columns\n",
    "def add_cosine_distance_column(df, col1, col2, new_col_name):\n",
    "    # Lowercase the text in both columns and fill NaNs with empty strings\n",
    "    col1_lower = df[col1].fillna('').str.lower()\n",
    "    col2_lower = df[col2].fillna('').str.lower()\n",
    "\n",
    "    # Compute cosine distances for each pair of strings\n",
    "    distances = [\n",
    "        1 - textdistance.cosine.normalized_similarity(text1, text2)\n",
    "        for text1, text2 in zip(col1_lower, col2_lower)\n",
    "    ]\n",
    "\n",
    "    # Add the new column with the computed distances\n",
    "    df[new_col_name] = distances\n",
    "\n",
    "# Add cosine distance columns for the specified pairs\n",
    "add_cosine_distance_column(deduped_df, 'env_broad_scale', 'envoBroadScale.label', 'cosine_distance_broad_scale')\n",
    "add_cosine_distance_column(deduped_df, 'env_local_scale', 'envoLocalScale.label', 'cosine_distance_local_scale')\n",
    "add_cosine_distance_column(deduped_df, 'env_medium', 'envoMedium.label', 'cosine_distance_medium')\n"
   ],
   "id": "a3e8c4622a84443b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:05:15.793326Z",
     "start_time": "2025-01-28T20:05:15.774853Z"
    }
   },
   "cell_type": "code",
   "source": "deduped_df.to_csv(merged_flagged_output_tsv, sep=\"\\t\", index=False)",
   "id": "2403e42de8ec21ee",
   "outputs": [],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
