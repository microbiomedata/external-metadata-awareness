{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:41.201948Z",
     "start_time": "2025-03-17T14:12:41.192667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "from oaklib import get_adapter\n",
    "from oaklib.datamodels.lexical_index import LexicalIndex\n",
    "from oaklib.datamodels.lexical_index import LexicalTransformationPipeline\n",
    "from oaklib.datamodels.lexical_index import RelationshipToTerm, LexicalGrouping, LexicalTransformation, \\\n",
    "    TransformationType\n",
    "from oaklib.datamodels.synonymizer_datamodel import Synonymizer\n",
    "from oaklib.interfaces.text_annotator_interface import TextAnnotatorInterface\n",
    "from oaklib.utilities.lexical.lexical_indexer import apply_transformation\n",
    "from oaklib.utilities.lexical.lexical_indexer import create_lexical_index\n",
    "from oaklib.utilities.lexical.lexical_indexer import save_lexical_index"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:41.566072Z",
     "start_time": "2025-03-17T14:12:41.563734Z"
    }
   },
   "cell_type": "code",
   "source": "envo_adapter_string  = \"sqlite:obo:envo\"",
   "id": "2c758748bf76b3b9",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:41.923829Z",
     "start_time": "2025-03-17T14:12:41.921541Z"
    }
   },
   "cell_type": "code",
   "source": "po_adapter_string  = \"sqlite:obo:po\"",
   "id": "741ec89f8626284e",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:42.269937Z",
     "start_time": "2025-03-17T14:12:42.265520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def are_pipelines_compatible(pipeline1, pipeline2):\n",
    "  \"\"\"\n",
    "  Check if two pipelines are functionally equivalent\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  pipeline1 : LexicalTransformationPipeline\n",
    "  pipeline2 : LexicalTransformationPipeline\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  bool\n",
    "      True if the pipelines are functionally equivalent\n",
    "  \"\"\"\n",
    "  # Check if they have the same number of transformations\n",
    "  if len(pipeline1.transformations) != len(pipeline2.transformations):\n",
    "      return False\n",
    "\n",
    "  # Check if each transformation is the same type and has the same parameters\n",
    "  for t1, t2 in zip(pipeline1.transformations, pipeline2.transformations):\n",
    "      # Get type values safely, handling different object structures\n",
    "      t1_type = getattr(t1.type, 'text', str(t1.type))\n",
    "      t2_type = getattr(t2.type, 'text', str(t2.type))\n",
    "\n",
    "      if t1_type != t2_type:\n",
    "          return False\n",
    "\n",
    "      # Compare params if they exist\n",
    "      t1_params = getattr(t1, 'params', [])\n",
    "      t2_params = getattr(t2, 'params', [])\n",
    "\n",
    "      # Simple length comparison for params\n",
    "      if len(t1_params) != len(t2_params):\n",
    "          return False\n",
    "\n",
    "  return True\n"
   ],
   "id": "5b8f805733f6c1a1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:42.293812Z",
     "start_time": "2025-03-17T14:12:42.288852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relationship_equals(rel1, rel2):\n",
    "  \"\"\"\n",
    "  Compare two relationships for functional equality\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  rel1 : RelationshipToTerm\n",
    "  rel2 : RelationshipToTerm\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  bool\n",
    "      True if relationships are functionally equivalent\n",
    "  \"\"\"\n",
    "  # Attributes that define uniqueness for a relationship\n",
    "  key_attrs = ['predicate', 'element', 'element_term', 'source']\n",
    "\n",
    "  return all(getattr(rel1, attr, None) == getattr(rel2, attr, None)\n",
    "            for attr in key_attrs)\n",
    "\n",
    "def deduplicate_lexical_index(lexical_index):\n",
    "  \"\"\"\n",
    "  Remove duplicate relationships from all groupings in a lexical index\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  lexical_index : LexicalIndex\n",
    "      The lexical index to deduplicate\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  LexicalIndex\n",
    "      The same lexical index with deduplicated relationships\n",
    "  \"\"\"\n",
    "  for term, grouping in lexical_index.groupings.items():\n",
    "      unique_relationships = []\n",
    "\n",
    "      # For each relationship, check if an equivalent one exists\n",
    "      for rel in grouping.relationships:\n",
    "          is_duplicate = False\n",
    "          for unique_rel in unique_relationships:\n",
    "              if relationship_equals(rel, unique_rel):\n",
    "                  is_duplicate = True\n",
    "                  break\n",
    "\n",
    "          if not is_duplicate:\n",
    "              unique_relationships.append(rel)\n",
    "\n",
    "      # Replace with deduplicated list\n",
    "      grouping.relationships = unique_relationships\n",
    "\n",
    "  return lexical_index"
   ],
   "id": "34e85179348a5a6a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:43.020625Z",
     "start_time": "2025-03-17T14:12:43.014637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_lexical_indexes(index1, index2, validate_pipelines=True):\n",
    "  \"\"\"\n",
    "  Merge two lexical indexes ensuring pipeline compatibility\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  index1 : LexicalIndex\n",
    "      First lexical index to merge\n",
    "  index2 : LexicalIndex\n",
    "      Second lexical index to merge\n",
    "  validate_pipelines : bool, default=True\n",
    "      If True, verify that similarly named pipelines are compatible\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  LexicalIndex\n",
    "      A new lexical index containing all entries from both inputs\n",
    "  \"\"\"\n",
    "  merged_index = LexicalIndex()\n",
    "\n",
    "  # Check pipeline compatibility if requested\n",
    "  if validate_pipelines:\n",
    "      for name, pipeline1 in index1.pipelines.items():\n",
    "          if name in index2.pipelines:\n",
    "              pipeline2 = index2.pipelines[name]\n",
    "              if not are_pipelines_compatible(pipeline1, pipeline2):\n",
    "                  raise ValueError(\n",
    "                      f\"Pipeline '{name}' is defined differently in the two indexes. \"\n",
    "                      \"Set validate_pipelines=False to override this check.\"\n",
    "                  )\n",
    "\n",
    "  # Copy pipelines from the first index\n",
    "  for name, pipeline in index1.pipelines.items():\n",
    "      merged_index.pipelines[name] = pipeline\n",
    "\n",
    "  # Add pipelines from second index (if not already present)\n",
    "  for name, pipeline in index2.pipelines.items():\n",
    "      if name not in merged_index.pipelines:\n",
    "          merged_index.pipelines[name] = pipeline\n",
    "\n",
    "  # Merge groupings - dict ensures no duplicate keys\n",
    "  for term, grouping in index1.groupings.items():\n",
    "      # Add the grouping directly with its relationships\n",
    "      merged_index.groupings[term] = LexicalGrouping(term=term)\n",
    "      merged_index.groupings[term].relationships = list(grouping.relationships)\n",
    "\n",
    "  # Add or merge groupings from the second index\n",
    "  for term, grouping in index2.groupings.items():\n",
    "      if term not in merged_index.groupings:\n",
    "          # New term, add the grouping directly\n",
    "          merged_index.groupings[term] = LexicalGrouping(term=term)\n",
    "          merged_index.groupings[term].relationships = list(grouping.relationships)\n",
    "      else:\n",
    "          # Term exists, append relationships\n",
    "          merged_index.groupings[term].relationships.extend(grouping.relationships)\n",
    "\n",
    "  # Deduplicate relationships\n",
    "  return deduplicate_lexical_index(merged_index)\n"
   ],
   "id": "259ef5076c40498e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:24:20.671803Z",
     "start_time": "2025-03-17T14:24:20.649856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_obsolete_terms_to_lexical_index(oi, lexical_index):\n",
    "    \"\"\"\n",
    "    Find all obsolete classes in an ontology, removes the 'obsolete ' prefix\n",
    "    from their labels, and adds them to an existing lexical index.\n",
    "    Also applies punctuation normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oi : BasicOntologyInterface\n",
    "        The ontology interface to search for obsolete classes\n",
    "    lexical_index : LexicalIndex\n",
    "        The existing lexical index to add the processed obsolete terms to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LexicalIndex\n",
    "        The updated lexical index with obsolete terms added\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all obsolete classes using the proper method\n",
    "    obsolete_classes = list(oi.obsoletes())\n",
    "    print(f\"Found {len(obsolete_classes)} obsolete classes\")\n",
    "\n",
    "    # Define punctuation normalization rule\n",
    "    punctuation_rule = Synonymizer(\n",
    "        description=\"Replace all punctuation with spaces\",\n",
    "        match=r\"[^\\w\\s]|_\",  # Match any non-alphanumeric, non-whitespace character or underscore\n",
    "        replacement=r\" \"  # Replace with a space\n",
    "    )\n",
    "\n",
    "    # Process each obsolete class\n",
    "    for obsolete_entity in obsolete_classes:\n",
    "        # Get the original label\n",
    "        orig_label = oi.label(obsolete_entity)\n",
    "        if not orig_label:\n",
    "            continue  # Skip entities without labels\n",
    "\n",
    "        # Remove \"obsolete \" prefix if present\n",
    "        if orig_label.lower().startswith(\"obsolete \"):\n",
    "            clean_label = re.sub(r'^obsolete\\s+', '', orig_label, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            clean_label = orig_label\n",
    "\n",
    "        # Process the main label first with all pipelines\n",
    "        for pipeline_name, pipeline in lexical_index.pipelines.items():\n",
    "            # Start with the clean label (obsolete prefix removed)\n",
    "            transformed_label = clean_label\n",
    "\n",
    "            # First apply punctuation normalization\n",
    "            punctuation_transformation = LexicalTransformation(\n",
    "                TransformationType.Synonymization,\n",
    "                params=[punctuation_rule]\n",
    "            )\n",
    "            result = apply_transformation(transformed_label, punctuation_transformation)\n",
    "            if isinstance(result, tuple):\n",
    "                transformed_label = result[1]\n",
    "            else:\n",
    "                transformed_label = result\n",
    "\n",
    "            # Then apply the pipeline's regular transformations\n",
    "            for transformation in pipeline.transformations:\n",
    "                result = apply_transformation(transformed_label, transformation)\n",
    "                if isinstance(result, tuple):\n",
    "                    transformed_label = result[1]\n",
    "                else:\n",
    "                    transformed_label = result\n",
    "\n",
    "            # Create relationship for the main label\n",
    "            rel = RelationshipToTerm(\n",
    "                predicate='rdfs:label',\n",
    "                element=obsolete_entity,\n",
    "                element_term=clean_label,\n",
    "                pipeline=[pipeline_name],\n",
    "                synonymized=False\n",
    "            )\n",
    "\n",
    "            # Add to lexical index\n",
    "            if transformed_label not in lexical_index.groupings:\n",
    "                lexical_index.groupings[transformed_label] = LexicalGrouping(term=transformed_label)\n",
    "                lexical_index.groupings[transformed_label].relationships = [rel]\n",
    "            else:\n",
    "                # Check if this relationship already exists to avoid duplicates\n",
    "                exists = False\n",
    "                for existing_rel in lexical_index.groupings[transformed_label].relationships:\n",
    "                    if (existing_rel.element == rel.element and\n",
    "                            existing_rel.predicate == rel.predicate):\n",
    "                        exists = True\n",
    "                        break\n",
    "\n",
    "                if not exists:\n",
    "                    lexical_index.groupings[transformed_label].relationships.append(rel)\n",
    "\n",
    "        # Now process all other aliases\n",
    "        alias_map = oi.entity_alias_map(obsolete_entity)\n",
    "\n",
    "        # For each pipeline and each alias (skipping the label we just processed)\n",
    "        for pipeline_name, pipeline in lexical_index.pipelines.items():\n",
    "            for predicate, aliases in alias_map.items():\n",
    "                for alias in aliases:\n",
    "                    # Skip the main label as we already processed it\n",
    "                    if alias == orig_label and predicate == 'rdfs:label':\n",
    "                        continue\n",
    "\n",
    "                    # Remove \"obsolete \" prefix if present\n",
    "                    if alias.lower().startswith(\"obsolete \"):\n",
    "                        clean_alias = re.sub(r'^obsolete\\s+', '', alias, flags=re.IGNORECASE)\n",
    "                    else:\n",
    "                        clean_alias = alias\n",
    "\n",
    "                    # Apply transformations including punctuation normalization\n",
    "                    transformed_alias = clean_alias\n",
    "\n",
    "                    # First apply punctuation normalization\n",
    "                    result = apply_transformation(transformed_alias, punctuation_transformation)\n",
    "                    if isinstance(result, tuple):\n",
    "                        transformed_alias = result[1]\n",
    "                    else:\n",
    "                        transformed_alias = result\n",
    "\n",
    "                    # Then apply the pipeline's regular transformations\n",
    "                    for transformation in pipeline.transformations:\n",
    "                        result = apply_transformation(transformed_alias, transformation)\n",
    "                        if isinstance(result, tuple):\n",
    "                            transformed_alias = result[1]\n",
    "                        else:\n",
    "                            transformed_alias = result\n",
    "\n",
    "                    # Create relationship for the alias\n",
    "                    rel = RelationshipToTerm(\n",
    "                        predicate=predicate,\n",
    "                        element=obsolete_entity,\n",
    "                        element_term=clean_alias,\n",
    "                        pipeline=[pipeline_name],\n",
    "                        synonymized=False\n",
    "                    )\n",
    "\n",
    "                    # Add to lexical index\n",
    "                    if transformed_alias not in lexical_index.groupings:\n",
    "                        lexical_index.groupings[transformed_alias] = LexicalGrouping(term=transformed_alias)\n",
    "                        lexical_index.groupings[transformed_alias].relationships = [rel]\n",
    "                    else:\n",
    "                        # Check if this relationship already exists to avoid duplicates\n",
    "                        exists = False\n",
    "                        for existing_rel in lexical_index.groupings[transformed_alias].relationships:\n",
    "                            if (existing_rel.element == rel.element and\n",
    "                                    existing_rel.predicate == rel.predicate):\n",
    "                                exists = True\n",
    "                                break\n",
    "\n",
    "                        if not exists:\n",
    "                            lexical_index.groupings[transformed_alias].relationships.append(rel)\n",
    "\n",
    "    # Apply deduplication to the entire index\n",
    "    return deduplicate_lexical_index(lexical_index)\n"
   ],
   "id": "98814641a18c2361",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:13:15.593649Z",
     "start_time": "2025-03-17T14:13:15.588497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_punctuation_insensitive_index(oi):\n",
    "  \"\"\"\n",
    "  Create a lexical index that is insensitive to punctuation by replacing all punctuation with spaces\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  oi : BasicOntologyInterface\n",
    "      The ontology interface\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  LexicalIndex\n",
    "      A lexical index with punctuation normalization\n",
    "  \"\"\"\n",
    "  # Define synonymizer rules for punctuation\n",
    "  punctuation_rules = [\n",
    "      # Replace all punctuation with spaces\n",
    "      # This covers hyphens, periods, commas, semicolons, etc.\n",
    "      Synonymizer(\n",
    "          description=\"Replace all punctuation with spaces\",\n",
    "          match=r\"[^\\w\\s]|_\",  # Match any non-alphanumeric, non-whitespace character or underscore\n",
    "          replacement=r\" \"      # Replace with a space\n",
    "      )\n",
    "  ]\n",
    "\n",
    "  # Create a pipeline with punctuation normalization\n",
    "  pipeline = LexicalTransformationPipeline(\n",
    "      name=\"punctuation_insensitive\",\n",
    "      transformations=[\n",
    "          # First apply synonymization for punctuation\n",
    "          LexicalTransformation(TransformationType.Synonymization, params=punctuation_rules),\n",
    "          # Then standard normalizations - WhitespaceNormalization will consolidate multiple spaces\n",
    "          LexicalTransformation(TransformationType.CaseNormalization),\n",
    "          LexicalTransformation(TransformationType.WhitespaceNormalization)\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  # Create the lexical index with our pipeline and rules\n",
    "  lexical_index = create_lexical_index(\n",
    "      oi,\n",
    "      pipelines=[pipeline],\n",
    "      synonym_rules=punctuation_rules\n",
    "  )\n",
    "\n",
    "  return lexical_index\n",
    "\n"
   ],
   "id": "73d920345d01fc97",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:43.401099Z",
     "start_time": "2025-03-17T14:12:43.397527Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 41,
   "source": "envo_adapter = get_adapter(envo_adapter_string)",
   "id": "c064d6956773af0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:12:43.750342Z",
     "start_time": "2025-03-17T14:12:43.747516Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 42,
   "source": "po_adapter = get_adapter(envo_adapter_string)",
   "id": "78c0c6ca229d67de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:13:14.139194Z",
     "start_time": "2025-03-17T14:13:14.119919Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 50,
   "source": [
    "# envo_po_tai = TextAnnotatorInterface()\n",
    "# envo_po_tai.lexical_index = updated_index"
   ],
   "id": "e0c8e99ad9c13289"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:13:14.479173Z",
     "start_time": "2025-03-17T14:13:14.476504Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 51,
   "source": "# annotation_results = envo_po_tai.annotate_text(\"the is some agricultural soil in my desert biome. I better check the root system width in my human associated habitat\")",
   "id": "63dae0dd45cfbadd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:13:14.851727Z",
     "start_time": "2025-03-17T14:13:14.845529Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 52,
   "source": "# annotation_results = list(annotation_results)",
   "id": "72f9e46008f09854"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:13:15.574111Z",
     "start_time": "2025-03-17T14:13:15.569882Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextAnnotation(predicate_id='oio:hasBroadSynonym', object_id='ENVO:00000172', object_label='desert', object_categories=[], object_source=None, confidence=None, match_string='desert', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=42, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='oio:hasBroadSynonym', object_id='ENVO:00000173', object_label='desert', object_categories=[], object_source=None, confidence=None, match_string='desert', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=42, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='oio:hasBroadSynonym', object_id='ENVO:00000183', object_label='desert', object_categories=[], object_source=None, confidence=None, match_string='desert', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=42, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='oio:hasRelatedSynonym', object_id='ENVO:00000315', object_label='desert', object_categories=[], object_source=None, confidence=None, match_string='desert', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=42, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:01001357', object_label='desert', object_categories=[], object_source=None, confidence=None, match_string='desert', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=42, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:00000428', object_label='biome', object_categories=[], object_source=None, confidence=None, match_string='biome', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=44, subject_end=48, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:00001998', object_label='soil', object_categories=[], object_source=None, confidence=None, match_string='soil', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=26, subject_end=29, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:00002259', object_label='agricultural soil', object_categories=[], object_source=None, confidence=None, match_string='agricultural soil', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=13, subject_end=29, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:01000179', object_label='desert biome', object_categories=[], object_source=None, confidence=None, match_string='desert biome', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=37, subject_end=48, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='ENVO:01000739', object_label='habitat', object_categories=[], object_source=None, confidence=None, match_string='habitat', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=111, subject_end=117, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='NCBITaxon:1', object_label='root', object_categories=[], object_source=None, confidence=None, match_string='root', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=70, subject_end=73, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='PO:0009005', object_label='root', object_categories=[], object_source=None, confidence=None, match_string='root', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=70, subject_end=73, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='oio:hasExactSynonym', object_id='NCBITaxon:9606', object_label='human', object_categories=[], object_source=None, confidence=None, match_string='human', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=94, subject_end=98, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='PATO:0000921', object_label='width', object_categories=[], object_source=None, confidence=None, match_string='width', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=82, subject_end=86, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='PO:0025025', object_label='root system', object_categories=[], object_source=None, confidence=None, match_string='root system', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=70, subject_end=80, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='rdfs:label', object_id='RO:0002577', object_label='system', object_categories=[], object_source=None, confidence=None, match_string='system', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=75, subject_end=80, subject_label=None, subject_source=None, subject_text_id=None),\n",
       " TextAnnotation(predicate_id='oio:hasBroadSynonym', object_id='UBERON:0000467', object_label='system', object_categories=[], object_source=None, confidence=None, match_string='system', is_longest_match=None, matches_whole_text=False, match_type=None, info=None, object_aliases=[], subject_start=75, subject_end=80, subject_label=None, subject_source=None, subject_text_id=None)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53,
   "source": "# annotation_results",
   "id": "69c376e7f40e11fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:27:52.678103Z",
     "start_time": "2025-03-17T14:27:41.997689Z"
    }
   },
   "cell_type": "code",
   "source": "envo_pi_ix = create_punctuation_insensitive_index(envo_adapter)",
   "id": "22b57a7993b73d3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Skipping statements(subject=ENVO:01001644,predicate=oio:hasDbXref,object=None,value=Carbonate which is formed as the result of some biological process.,datatype=None,language=None,); ValueError: Carbonate which is formed as the result of some biological process. is not a valid URI or CURIE\n",
      "WARNING:root:Skipping <http://geneontology.org/foo/applies-pattern> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <http://schema.org/image> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q2306597> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q2> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q525> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q715269> as it is not a valid CURIE\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:27:53.218946Z",
     "start_time": "2025-03-17T14:27:52.725577Z"
    }
   },
   "cell_type": "code",
   "source": "envo_pi_with_obsoletes_ix = add_obsolete_terms_to_lexical_index(envo_adapter, envo_pi_ix)",
   "id": "f59957033d084d9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 460 obsolete classes\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:28:05.214227Z",
     "start_time": "2025-03-17T14:27:53.633062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# annotation_results\n",
    "po_pi_ix = create_punctuation_insensitive_index(po_adapter)"
   ],
   "id": "a63c138c0bf36860",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Skipping statements(subject=ENVO:01001644,predicate=oio:hasDbXref,object=None,value=Carbonate which is formed as the result of some biological process.,datatype=None,language=None,); ValueError: Carbonate which is formed as the result of some biological process. is not a valid URI or CURIE\n",
      "WARNING:root:Skipping <http://geneontology.org/foo/applies-pattern> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <http://schema.org/image> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q2306597> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q2> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q525> as it is not a valid CURIE\n",
      "WARNING:root:Skipping <https://www.wikidata.org/wiki/Q715269> as it is not a valid CURIE\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:29:04.767266Z",
     "start_time": "2025-03-17T14:29:04.570759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "envo_po_ix = None\n",
    "\n",
    "try:\n",
    "    # Merge with pipeline validation\n",
    "    envo_po_ix = merge_lexical_indexes(envo_pi_with_obsoletes_ix, po_pi_ix)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Merging failed: {e}\")\n",
    "    # Handle pipeline incompatibility\n"
   ],
   "id": "17869a09ec5ab4c5",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:29:14.018941Z",
     "start_time": "2025-03-17T14:29:06.047274Z"
    }
   },
   "cell_type": "code",
   "source": "save_lexical_index(envo_po_ix, \"envo_inc_obsoletes_po_punct_free_index.yaml\")",
   "id": "dd380f6ca001bd0",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "166fe962b5e3f1b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
