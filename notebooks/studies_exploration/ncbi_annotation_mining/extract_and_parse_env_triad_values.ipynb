{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ~ 6 minutes, 2025-03-08",
   "id": "7e8e57f1d524e077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"\n",
    "    Normalizes a string by lowercasing, trimming, and removing duplicate whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"  # handle non string input.\n",
    "    s = s.lower()\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)  # Replace multiple whitespace with single space\n",
    "    return s"
   ],
   "id": "25dcbd35294b4611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def repair_curie(raw_curie: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempt to normalize or 'repair' the CURIE-like string:\n",
    "      1. Normalize namespace to uppercase if it looks like 'ENVO'.\n",
    "      2. Replace underscores or periods with a colon (':') between prefix & ID.\n",
    "      3. Strip extra whitespace around the ID.\n",
    "    \"\"\"\n",
    "    curie = raw_curie.strip()\n",
    "\n",
    "    # Attempt to split at the first occurrence of [.:_]\n",
    "    match = re.match(r'^([A-Za-z0-9]+)([\\.:_])\\s*(.*)$', curie) # todo not sensitive enough to the length and letter/number composition of prefix and local portion\n",
    "    if not match:\n",
    "        # If it doesn't match, just return the stripped version\n",
    "        return curie\n",
    "\n",
    "    prefix, sep, rest = match.groups()\n",
    "\n",
    "    # Rebuild as prefix:rest with no extra spaces\n",
    "    # todo repair should include normalizing the case of the prefix\n",
    "    repaired = f\"{prefix}:{rest.strip()}\" # todo excessive replacement of underscores with colons in insensitive curies above\n",
    "    return repaired"
   ],
   "id": "e267b0282e232884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_and_parse_env_triad_values(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"biosample_harmonized_attributes\",\n",
    "        output_collection_name=\"unique_triad_values\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts unique values from 'env_broad_scale', 'env_local_scale', and 'env_medium' attributes\n",
    "    from a MongoDB collection, counts their occurrences, and stores them in a new collection.\n",
    "    Adds a progress indicator using estimated document count.\n",
    "\n",
    "    :param mongo_uri: MongoDB connection URI.\n",
    "    :param db_name: Name of the database.\n",
    "    :param input_collection_name: Name of the input collection.\n",
    "    :param output_collection_name: Name of the output collection.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        client = MongoClient(mongo_uri)\n",
    "        db = client[db_name]\n",
    "        input_collection = db[input_collection_name]\n",
    "        output_collection = db[output_collection_name]\n",
    "\n",
    "        output_collection.drop()  # Drop the collection\n",
    "\n",
    "        print(f\"Collection '{output_collection_name}' dropped successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    try:\n",
    "        unique_values = {}\n",
    "\n",
    "        estimated_count = input_collection.estimated_document_count()\n",
    "        print(f\"{estimated_count = }\")\n",
    "\n",
    "        with tqdm(total=estimated_count, desc=\"Processing Harmonized Biosamples\") as pbar:\n",
    "            for sample in input_collection.find():\n",
    "                env_broad = sample.get(\"env_broad_scale\")\n",
    "                env_local = sample.get(\"env_local_scale\")\n",
    "                env_medium = sample.get(\"env_medium\")\n",
    "\n",
    "                for value in [env_broad, env_local, env_medium]:\n",
    "                    if value:\n",
    "                        if value in unique_values:\n",
    "                            unique_values[value][\"count\"] += 1\n",
    "                        else:\n",
    "                            unique_values[value] = {\"content\": value, \"count\": 1}\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Insert unique values into the new collection\n",
    "        output_collection.insert_many(list(unique_values.values()))\n",
    "\n",
    "        print(\n",
    "            f\"Extracted and counted unique environment attributes from '{input_collection_name}' to '{output_collection_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'client' in locals() and client:\n",
    "            client.close()"
   ],
   "id": "f597617f0b020858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_count_distribution(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"unique_triad_values\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves counts from a MongoDB collection and displays their distribution as a histogram.\n",
    "\n",
    "    :param mongo_uri: MongoDB connection URI.\n",
    "    :param db_name: Name of the database.\n",
    "    :param input_collection_name: Name of the collection containing counts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(mongo_uri)\n",
    "        db = client[db_name]\n",
    "        collection = db[input_collection_name]\n",
    "\n",
    "        counts = [doc[\"count\"] for doc in collection.find({}, {\"count\": 1, \"_id\": 0})]  # get all counts\n",
    "\n",
    "        if not counts:\n",
    "            print(\"No counts found in the collection.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(counts, bins=np.logspace(np.log10(min(counts)), np.log10(max(counts)), 50),\n",
    "                 log=True)  # use log scale and logspace for bins.\n",
    "        plt.xscale('log')\n",
    "        plt.title(\"Distribution of Counts\")\n",
    "        plt.xlabel(\"Count (log scale)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'client' in locals() and client:\n",
    "            client.close()\n",
    "\n"
   ],
   "id": "3201bd58af86a90d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def assess_normalization_impact(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"unique_triad_values\",\n",
    "        fields_to_normalize=[\"content\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Assesses the impact of string normalization on specified fields in a MongoDB collection.\n",
    "\n",
    "    :param mongo_uri: MongoDB connection URI.\n",
    "    :param db_name: Name of the database.\n",
    "    :param input_collection_name: Name of the collection.\n",
    "    :param fields_to_normalize: List of fields to normalize and assess.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(mongo_uri)\n",
    "        db = client[db_name]\n",
    "        collection = db[input_collection_name]\n",
    "\n",
    "        original_unique_counts = {}\n",
    "        normalized_unique_counts = {}\n",
    "\n",
    "        for field in fields_to_normalize:\n",
    "            original_unique_values = set()\n",
    "            normalized_unique_values = set()\n",
    "\n",
    "            for doc in collection.find({field: {\"$ne\": None}}, {field: 1, \"_id\": 0}):\n",
    "                original_value = doc.get(field)\n",
    "                if isinstance(original_value, str):\n",
    "                    original_unique_values.add(original_value)\n",
    "                    normalized_value = normalize_string(original_value)\n",
    "                    normalized_unique_values.add(normalized_value)\n",
    "\n",
    "            original_unique_counts[field] = len(original_unique_values)\n",
    "            normalized_unique_counts[field] = len(normalized_unique_values)\n",
    "\n",
    "        print(\"Unique value counts before normalization:\")\n",
    "        for field, count in original_unique_counts.items():\n",
    "            print(f\"  {field}: {count}\")\n",
    "\n",
    "        print(\"\\nUnique value counts after normalization:\")\n",
    "        for field, count in normalized_unique_counts.items():\n",
    "            print(f\"  {field}: {count}\")\n",
    "\n",
    "        print(\"\\nReduction in unique values:\")\n",
    "        for field in fields_to_normalize:\n",
    "            reduction = original_unique_counts[field] - normalized_unique_counts[field]\n",
    "            percentage_reduction = (reduction / original_unique_counts[field]) * 100 if original_unique_counts[\n",
    "                                                                                            field] > 0 else 0\n",
    "            print(f\"  {field}: {reduction} ({percentage_reduction:.2f}%)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'client' in locals() and client:\n",
    "            client.close()\n"
   ],
   "id": "d658ba1742de511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_envo_values(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"unique_triad_values\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds all content values containing \"envo\" (case-insensitive) in a MongoDB collection.\n",
    "\n",
    "    :param mongo_uri: MongoDB connection URI.\n",
    "    :param db_name: Name of the database.\n",
    "    :param input_collection_name: Name of the collection.\n",
    "    :return: A list of content values containing \"envo\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(mongo_uri)\n",
    "        db = client[db_name]\n",
    "        collection = db[input_collection_name]\n",
    "\n",
    "        envo_values = []\n",
    "        for doc in collection.find():\n",
    "            content = doc.get(\"content\", \"\")\n",
    "            if \"envo\" in content.lower():\n",
    "                envo_values.append(content)\n",
    "\n",
    "        return envo_values\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'client' in locals() and client:\n",
    "            client.close()\n"
   ],
   "id": "8f3056e2cdec8ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dump_list_to_file(string_list, filename):\n",
    "  \"\"\"\n",
    "  Dumps a list of strings to a text file, one item per line.\n",
    "\n",
    "  Args:\n",
    "    string_list: The list of strings to dump.\n",
    "    filename: The name of the file to create or overwrite.\n",
    "  \"\"\"\n",
    "  with open(filename, 'w') as f:\n",
    "    for item in string_list:\n",
    "      f.write(item + '\\n')\n"
   ],
   "id": "7ce4f856ba6ade10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_annotations(raw_content: str, obo_prefixes_lc: list, bioportal_prefixes_lc: list):\n",
    "    \"\"\"\n",
    "    Split a single raw line into sub-annotations, returning a list of dicts:\n",
    "      [\n",
    "         {\n",
    "           'raw_chunk': \"...\",         # substring after splitting\n",
    "           'raw_curie': \"...\",         # curie as found in text (if any)\n",
    "           'repaired_curie': \"...\",    # normalized curie (if any)\n",
    "           'raw_label': \"...\",         # substring minus the matched curie portion\n",
    "           'cleaned_label': \"...\"      # lowercased + extra whitespace removed\n",
    "         },\n",
    "         ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    # Split on major delimiters (pipe, semicolon, slash, comma)\n",
    "    components = re.split(DELIMITERS, raw_content)\n",
    "\n",
    "    results = []\n",
    "    for component in components:\n",
    "        component = component.strip()\n",
    "        if not component:\n",
    "            continue\n",
    "\n",
    "        match = CURIE_PATTERN.search(component) # todo pattern is over-eager in detecting CURIes\n",
    "        if match:\n",
    "            raw_curie = match.group(1)\n",
    "            repaired = repair_curie(raw_curie)\n",
    "\n",
    "            # Remove the matched substring from the chunk to guess the label\n",
    "            label_guess = component[:match.start()] + component[match.end():]\n",
    "            # Also remove leftover brackets or parentheses\n",
    "            label_guess = re.sub(r'[\\(\\)\\[\\]]+', ' ', label_guess).strip()\n",
    "\n",
    "            cleaned_label = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", label_guess.lower())\n",
    "            cleaned_label = re.sub(f\"\\d+\", \" \", cleaned_label)\n",
    "            cleaned_label = re.sub(r'\\s+', ' ', cleaned_label.strip())\n",
    "\n",
    "        else:\n",
    "            # No curie found; treat entire chunk as label\n",
    "            raw_curie = ''\n",
    "            repaired = ''\n",
    "            cleaned_label = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", component.lower())\n",
    "            cleaned_label = re.sub(f\"\\d+\", \" \", cleaned_label)\n",
    "            cleaned_label = re.sub(r'\\s+', ' ', cleaned_label.strip())\n",
    "            label_guess = component\n",
    "\n",
    "        repaired_prefix = repaired.split(\":\")[0]\n",
    "        repaired_prefix_lc = repaired_prefix.lower()\n",
    "\n",
    "        results.append({\n",
    "            'raw_component': component,\n",
    "            'raw_curie': raw_curie,\n",
    "            'repaired_curie': repaired,\n",
    "            'repaired_prefix': repaired_prefix,\n",
    "            'obo_prefix': repaired_prefix_lc in obo_prefixes_lc,\n",
    "            'bioportal_prefix': repaired_prefix_lc in bioportal_prefixes_lc,\n",
    "            'raw_label': label_guess,\n",
    "            'cleaned_label': cleaned_label,\n",
    "            'cleaned_label_len': len(cleaned_label),\n",
    "        })\n",
    "\n",
    "    return results"
   ],
   "id": "c0c2597167b013e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to get a random document by repaired prefix\n",
    "# todo assumes presence of collection object\n",
    "def get_random_by_repaired_prefix(prefix, input_collection_object):\n",
    "    result = input_collection_object.aggregate([\n",
    "        {\"$match\": {\"parsed_annotations.repaired_prefix\": prefix}},\n",
    "        {\"$sample\": {\"size\": 1}}\n",
    "    ])\n",
    "    return list(result)  # Convert cursor to list for easy access\n"
   ],
   "id": "298d3cd55dc4c7e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to get a random document by annotations count\n",
    "# todo assumes presence of collection object\n",
    "def get_random_by_annotations_count(count, input_collection_object):\n",
    "    result = input_collection_object.aggregate([\n",
    "        {\"$match\": {\"annotations_count\": count}},\n",
    "        {\"$sample\": {\"size\": 1}}\n",
    "    ])\n",
    "    return list(result)  # Convert cursor to list for easy access"
   ],
   "id": "aff9c947d99e262c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating a collection of all values observed in any one of the triad fields, with biosample usage counts\n",
    "extract_and_parse_env_triad_values(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"biosample_harmonized_attributes\",\n",
    "        output_collection_name=\"unique_triad_values\"\n",
    ")\n",
    "# 6 minutes"
   ],
   "id": "4e7d14c24402220b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage (assuming MongoDB is running locally)\n",
    "display_count_distribution(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\",\n",
    "        db_name=\"ncbi_metadata\",\n",
    "        input_collection_name=\"unique_triad_values\"\n",
    ")"
   ],
   "id": "bf6687d3b502d2ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## There are limits to the usefulness of premature normalization",
   "id": "b8f29fe2c4911a09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "assess_normalization_impact(\n",
    "    mongo_uri=\"mongodb://localhost:27017/\",\n",
    "    db_name=\"ncbi_metadata\",\n",
    "    input_collection_name=\"unique_triad_values\",\n",
    "    fields_to_normalize=[\"content\"]\n",
    ")"
   ],
   "id": "d3ad3ca7f4dbb8e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "b3ae4dfa27fcfb34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "771c98a5928ac995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## For analyzing with LLMs etc.# Example usage:\n",
    "envo_values = find_envo_values(\n",
    "    mongo_uri=\"mongodb://localhost:27017/\",\n",
    "    db_name=\"ncbi_metadata\",\n",
    "    input_collection_name=\"unique_triad_values\"\n",
    ")"
   ],
   "id": "7c4c43614728381f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(envo_values))",
   "id": "895fdaff38e434fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "triad_values_containing_envo_txt = \"triad_values_containing_envo.txt\"",
   "id": "621bc837678f2756",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dump_list_to_file(envo_values, triad_values_containing_envo_txt)",
   "id": "b6bdca54e535b2fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "b5da5e46640797c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load data about ontologies\n",
    "# may come from other notebooks"
   ],
   "id": "7c2e7c94de6a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "obo_ontologies_tsv = \"../../multi-lexmatch/obo_ontologies.tsv\"\n",
    "bioportal_ontology_class_counts_tsv = \"../../multi-lexmatch/bioportal_ontology_class_counts.tsv\"\n"
   ],
   "id": "9003dd3eaacffc2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "obo_ontologies_frame = pd.read_csv(obo_ontologies_tsv, sep=\"\\t\")",
   "id": "99a94d797a78ccd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "obo_prefixes = set(obo_ontologies_frame['id']).union(set(obo_ontologies_frame['preferredPrefix']))\n",
    "obo_prefixes_lc = {str(i).lower() for i in obo_prefixes}"
   ],
   "id": "bd8ae63be0f6081d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bioportal_ontologies =  pd.read_csv(bioportal_ontology_class_counts_tsv, sep=\"\\t\")",
   "id": "25ebeeb836a1307a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bioportal_prefixes = set(bioportal_ontologies['acronym'])\n",
    "bioportal_prefixes_lc = {i.lower() for i in bioportal_prefixes}"
   ],
   "id": "8b8b359f65133ce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "legit_prefixes_lc = list(obo_prefixes_lc.union(bioportal_prefixes_lc))\n",
    "legit_prefixes_lc.sort()"
   ],
   "id": "e23fcd03a34daaf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----\n",
   "id": "534f74e78e493ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Regex to find potential CURIEs, optionally bracketed or parenthesized\n",
    "# e.g., (ENVO:00000446), [EnvO_00005774], (envo.123456), etc.\n",
    "\n",
    "# todo: over eager. doesn't take into consideration the typical length and character/number composition of real CURIes\n",
    "CURIE_PATTERN = re.compile(\n",
    "    r'[\\(\\[\\s]*([A-Za-z0-9]+[\\.:_]\\s*[0-9A-Za-z]+)\\s*[\\)\\]\\s]*',\n",
    "    flags=re.IGNORECASE\n",
    ")"
   ],
   "id": "65b9d1427234fdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We consider these delimiters for splitting multiple annotations in one value\n",
    "DELIMITERS = r'[|;,/]+'"
   ],
   "id": "9d8ed58fe52b6525",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Connect to local MongoDB on default port\n",
    "client = MongoClient('mongodb://localhost:27017')\n"
   ],
   "id": "553e54eb50dfcc7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Access the database and the collections\n",
    "# use the connection builder from core.py>\n",
    "db = client.ncbi_metadata\n",
    "triad_values_collection = db.unique_triad_values\n"
   ],
   "id": "46bc95d9ae77a1db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For each document in unique_triad_values, parse and upsert into unique_triad_values\n",
    "\n",
    "for doc in triad_values_collection.find():\n",
    "    original_content = doc.get(\"content\", \"\")\n",
    "    parsed_annotations = split_annotations(original_content, obo_prefixes_lc=obo_prefixes_lc,\n",
    "                                           bioportal_prefixes_lc=bioportal_prefixes_lc)\n",
    "\n",
    "    # Create the update document\n",
    "    new_doc = {\n",
    "        \"content_len\": len(original_content),\n",
    "        \"formula_like\": original_content.startswith(\"=\"),\n",
    "        \"parsed_annotations\": parsed_annotations,\n",
    "        \"annotations_count\": len(parsed_annotations),\n",
    "    }\n",
    "\n",
    "    # Upsert (update if exists, insert if not)\n",
    "    triad_values_collection.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},  # Match by original document ID\n",
    "        {\"$set\": new_doc},  # Update fields\n",
    "        upsert=True  # Insert if doesn't exist\n",
    "    )\n",
    "\n",
    "print(\"Upserted parsed triad annotations.\")\n",
    "\n",
    "# ~ 1 minute\n"
   ],
   "id": "70362d048a74ef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure indexes exist for efficient querying\n",
    "triad_values_collection.create_index([(\"annotations_count\", 1)])\n",
    "triad_values_collection.create_index([(\"content\", 1)])\n",
    "triad_values_collection.create_index([(\"content_len\", 1)])\n",
    "triad_values_collection.create_index([(\"count\", 1)])\n",
    "triad_values_collection.create_index([(\"formula_like\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.cleaned_label\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.cleaned_label_len\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.raw_component\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.raw_curie\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.raw_label\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.repaired_curie\", 1)])\n",
    "triad_values_collection.create_index([(\"parsed_annotations.repaired_prefix\", 1)])"
   ],
   "id": "e57324f303ddf58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo check for presence in obo foundry registry yaml\n",
    "# https://raw.githubusercontent.com/OBOFoundry/OBOFoundry.github.io/refs/heads/master/registry/ontologies.yml\n",
    "# aka http://purl.obolibrary.org/meta/ontologies.yml\n",
    "# parsed by notebooks/multi-lexmatch/parse_obo_ontologies_yaml.ipynb\n",
    "# into notebooks/multi-lexmatch/obo_ontologies.tsv\n",
    "# could also check analogous notebooks/multi-lexmatch/bioportal_ontology_class_counts.tsv"
   ],
   "id": "9e6f4e2e9acc21c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_annotations = triad_values_collection.find({}, {\"parsed_annotations.repaired_prefix\": 1})\n",
    "\n",
    "repaired_prefix_list = [\n",
    "    annotation[\"repaired_prefix\"]\n",
    "    for doc in all_annotations\n",
    "    for annotation in doc.get(\"parsed_annotations\", [])\n",
    "    if annotation[\"repaired_prefix\"]  # Exclude empty values\n",
    "]\n",
    "\n",
    "prefix_counts = Counter(repaired_prefix_list)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "prefix_count_frame = pd.DataFrame(prefix_counts.items(), columns=[\"repaired_prefix\", \"Count\"])\n",
    "prefix_count_frame = prefix_count_frame.sort_values(by=\"Count\", ascending=False)\n"
   ],
   "id": "65a5becfc77197e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prefix_count_frame[\"obo_or_bioportal\"] = prefix_count_frame[\"repaired_prefix\"].str.lower().isin(legit_prefixes_lc)",
   "id": "feecb3ab54220d71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "some ontologies that we don't talk about frequently:\n",
    "- bto brenda tissue ontology\n",
    "- opl Ontology for Parasite LifeCycle\n",
    "- pco Population and Community Ontology\n",
    "- mmo measurement method ontology"
   ],
   "id": "60e53f90b1e5a074"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prefix_count_frame",
   "id": "4aa465c0795e0f75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "6f96a637a57753a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve all annotation count values\n",
    "all_counts = triad_values_collection.find({}, {\"annotations_count\": 1})\n",
    "\n",
    "# Extract annotation counts into a list\n",
    "annotation_count_list = [\n",
    "    doc.get(\"annotations_count\", 0) for doc in all_counts\n",
    "]\n",
    "\n",
    "# Count occurrences of each annotation count value\n",
    "annotation_count_distribution = Counter(annotation_count_list)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "annotation_count_frame = pd.DataFrame(\n",
    "    annotation_count_distribution.items(), columns=[\"Annotations Count\", \"Frequency\"]\n",
    ")\n",
    "annotation_count_frame = annotation_count_frame.sort_values(by=\"Annotations Count\", ascending=True)"
   ],
   "id": "7e0f94f5f60c6056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "annotation_count_frame",
   "id": "263823294b57d457",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_prefix_sample = get_random_by_repaired_prefix(\"ENVO\", input_collection_object=triad_values_collection)",
   "id": "bddcdf8435d93aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_prefix_sample",
   "id": "1517c33b51af1000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_count_sample = get_random_by_annotations_count(3, input_collection_object=triad_values_collection)",
   "id": "ab7531177010ff75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_count_sample",
   "id": "12f1c15b86e16686",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve all parsed annotations along with their counts\n",
    "all_annotations = triad_values_collection.find({}, {\"parsed_annotations.cleaned_label\": 1, \"count\": 1})\n",
    "\n",
    "# Dictionary to store cumulative counts for each cleaned_label\n",
    "cleaned_label_counts = Counter()\n",
    "\n",
    "# Iterate through each document and sum up counts per cleaned_label\n",
    "for doc in all_annotations:\n",
    "    doc_count = doc.get(\"count\", 1)  # Default count to 1 if missing (to avoid KeyErrors)\n",
    "\n",
    "    for annotation in doc.get(\"parsed_annotations\", []):\n",
    "        cleaned_label = annotation.get(\"cleaned_label\")\n",
    "        if cleaned_label:  # Ensure it's not empty\n",
    "            cleaned_label_counts[cleaned_label] += doc_count  # Sum based on document count"
   ],
   "id": "12aa1b1cb62250e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert Counter to a list of dictionaries\n",
    "component_label_counts = [{\"component_label\": key, \"count\": count} for key, count in cleaned_label_counts.items()]"
   ],
   "id": "fc22968696fa771f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "component_label_count_values = [i['count'] for i in component_label_counts]",
   "id": "b475c8e62eb51279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "triad_components_labels_collection = db.triad_components_labels",
   "id": "76c9c0432522dd1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Insert all records at once\n",
    "if component_label_counts:\n",
    "    triad_components_labels_collection.insert_many(component_label_counts)\n",
    "    print(\"Inserted records successfully!\")"
   ],
   "id": "13fe2288206d9c2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure indexes exist for efficient querying\n",
    "triad_components_labels_collection.create_index([(\"component_label\", 1)])\n",
    "triad_components_labels_collection.create_index([(\"count\", 1)])"
   ],
   "id": "baf4cb70b8ed340c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(component_label_count_values,\n",
    "         bins=np.logspace(np.log10(min(component_label_count_values)), np.log10(max(component_label_count_values)), 50),\n",
    "         log=True)  # use log scale and logspace for bins.\n",
    "plt.xscale('log')\n",
    "plt.title(\"Distribution of Counts\")\n",
    "plt.xlabel(\"Count (log scale)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "b4cfb2739bc4b7ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert to a DataFrame\n",
    "df_cleaned_label_counts = pd.DataFrame(cleaned_label_counts.items(), columns=[\"Cleaned Label\", \"Count\"])\n",
    "df_cleaned_label_counts = df_cleaned_label_counts.sort_values(by=\"Count\", ascending=False)"
   ],
   "id": "86cf5a96be5668a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_cleaned_label_counts",
   "id": "bc0fb5c2dc3abd12",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
