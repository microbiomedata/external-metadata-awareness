#!/usr/bin/env python3
"""
Prioritize Lexical Match Results from SSSOM TSV Files

This script processes SSSOM (Simple Standard for Sharing Ontology Mappings) TSV files 
generated by lexical matching tools, prioritizing mappings based on match quality.
It filters and ranks ontology mapping pairs to retain only the highest quality evidence
for each subject-object pair.

The script is designed to work with lexical match output files in the SSSOM format,
particularly those generated for the MetPO (Metabolomics Processing Ontology) project
and other ontology mapping efforts in the environmental metadata context.

Usage:
    python prioritize_lexmatch_results.py -i input.tsv -o output.tsv

SSSOM Format:
    The SSSOM format is a standardized TSV format for representing ontology mappings.
    It includes columns such as:
    - subject_id: The ID of the source ontology term
    - object_id: The ID of the target ontology term
    - subject_label/object_label: Human-readable labels
    - subject_match_field/object_match_field: Which fields were matched
    - mapping_justification: Reason for the mapping
    - confidence: Numeric confidence score

This script helps manage the potentially large number of matches produced by
lexical matching algorithms by selecting only the most reliable evidence.
"""

import click
import pandas as pd

# Define the evidence ranking; lower numbers indicate stronger evidence.
# This ranking prioritizes exact matches (rdfs:label) over synonyms and broader matches.
EVIDENCE_RANKING = {
    'rdfs:label': 0,              # Direct label match (strongest evidence)
    'oio:hasExactSynonym': 1,     # Exact synonym match 
    'oio:hasDbXref': 2,           # Database cross-reference
    'oio:hasRelatedSynonym': 3,   # Related synonym
    'skos:relatedMatch': 4,       # Related concept match
    'skos:relatedMatch-INVERSE': 5, # Inverse related match
    'oio:hasBroadSynonym': 6,     # Broader synonym
    'oio:hasNarrowSynonym': 7,    # Narrower synonym
    'rdf:ID': 8                   # ID-based match (weakest evidence)
}


@click.command()
@click.option('--input', '-i', 'input_file', required=True,
              help="Path to the input SSSOM TSV file")
@click.option('--output', '-o', 'output_file', required=True,
              help="Path for the output filtered TSV file")
def main(input_file, output_file):
    """
    Process SSSOM lexical matching files to retain only the highest quality evidence.
    
    For each subject-object pair in the input file, this script:
    1. Ranks all matches based on the evidence type (defined in EVIDENCE_RANKING)
    2. Keeps only the rows with the best evidence score for each subject-object pair
    3. Removes duplicate mappings and self-mappings (where subject_id = object_id)
    4. Sorts the results for better readability
    
    The filtered results provide a cleaner, more manageable set of mappings
    that prioritize the most reliable lexical matching evidence.
    """

    # Load the TSV file, ignoring lines that start with "#"
    df = pd.read_csv(input_file, sep="\t", comment='#')

    # Report the number of rows before filtering.
    start_rows = len(df)
    click.echo(f"Starting rows: {start_rows}")

    # Helper function to compute evidence score for a row.
    def get_evidence_score(row):
        """
        Calculate an evidence quality score for a mapping row.
        
        The function examines both the subject and object match fields and returns
        the better (lower) score based on the EVIDENCE_RANKING dictionary.
        
        Args:
            row: A pandas Series representing a row in the SSSOM TSV file
            
        Returns:
            int: The evidence score (lower is better), with 999 as default for unknown fields
        """
        subj_val = row.get('subject_match_field')
        obj_val = row.get('object_match_field')
        # Look up scores in EVIDENCE_RANKING, defaulting to 999 for missing/unknown fields
        score_subj = EVIDENCE_RANKING.get(subj_val, 999) if pd.notnull(subj_val) else 999
        score_obj = EVIDENCE_RANKING.get(obj_val, 999) if pd.notnull(obj_val) else 999
        # Return the better (lower) of the two scores
        return min(score_subj, score_obj)

    # Calculate evidence score for each row.
    df['evidence_score'] = df.apply(get_evidence_score, axis=1)

    # Group by subject and object.
    group_cols = ['subject_id', 'object_id']
    group_min = df.groupby(group_cols)['evidence_score'].min().reset_index()
    group_min = group_min.rename(columns={'evidence_score': 'min_evidence_score'})

    # Merge the minimum score into the main DataFrame.
    df = pd.merge(df, group_min, on=group_cols, how='left')

    # Keep only rows that have the best evidence score.
    filtered_df = df[df['evidence_score'] == df['min_evidence_score']].copy()

    # Remove duplicate rows.
    filtered_df = filtered_df.drop_duplicates()

    # Remove rows where subject_id equals object_id.
    filtered_df = filtered_df[filtered_df['subject_id'] != filtered_df['object_id']]

    # Drop helper columns.
    filtered_df = filtered_df.drop(columns=['evidence_score', 'min_evidence_score'])

    # Sort the results case-insensitively.
    filtered_df = filtered_df.sort_values(
        by=['subject_label', 'object_label', 'subject_match_field', 'object_match_field'],
        key=lambda col: col.str.lower()
    )

    # Report the number of rows after filtering.
    end_rows = len(filtered_df)
    click.echo(f"Ending rows: {end_rows}")

    # Save the filtered, sorted DataFrame to the output TSV file.
    filtered_df.to_csv(output_file, sep="\t", index=False)
    click.echo(f"Filtered file saved to: {output_file}")


if __name__ == '__main__':
    """
    Entry point for the script when run directly.
    
    Example usage:
        python prioritize_lexmatch_results.py -i metpo/metpo-cleanroom-lexmatch-SSSOM.tsv \
            -o metpo/metpo-cleanroom-lexmatch-SSSOM-prioritized.tsv
    
    This will process the lexical matching results in the input file and 
    save a filtered version that keeps only the highest quality matches
    between each pair of ontology terms.
    """
    main()
