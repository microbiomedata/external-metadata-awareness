# Architecture & Data Infrastructure

Technical reference for database systems, data flow, and infrastructure patterns.

---

## Table of Contents
- [MongoDB Infrastructure](#mongodb-infrastructure)
- [DuckDB & SQLite Usage](#duckdb--sqlite-usage)
- [Data Flow & Processing Pipeline](#data-flow--processing-pipeline)
- [External Data Sources](#external-data-sources)
- [Connection Patterns](#connection-patterns)

---

## MongoDB Infrastructure

### Servers
- **Development Server**: `localhost:27017` (default MongoDB port)
- **NMDC Server**: `localhost:27777` (with authentication required)
- **Load Balancer**: `mongo-ncbi-loadbalancer` (production instance)

### Primary Databases

#### 1. ncbi_metadata (~25GB local, ~50GB loadbalancer)
Main database storing NCBI-related data.

**Core Collections**:
- `biosamples`: 44M documents - Raw NCBI BioSample XML data
- `biosample_harmonized_attributes`: 44M documents - Flattened attributes
- `env_triads_flattened`: ~200K documents - Environmental context values
- `unique_triad_values`: 192K documents - Unique environmental values
- `class_label_cache`: 10K documents - Ontology class labels
- `triad_components_labels`: 46K documents - Environmental context labels

**BioProject Collections**:
- `bioprojects`: 893K documents (~1.5GB) - BioProject metadata
- `bioprojects_submissions`: 893K documents (~700MB) - Submission metadata

**Relationships**:
- `sra_biosamples_bioprojects`: 29M documents - SRA↔Biosample↔BioProject links

**Other Collections**:
- `filtered_sra_metadata`: 2.7M documents - Filtered SRA from BigQuery
- `simons_wishlist`: 62K documents - Curated bioprojects for analysis
- `packages`: 228 documents - NCBI package definitions

#### 2. biosamples (~29GB)
- `sra_metadata`: 35M documents (~108GB) - Complete SRA metadata
- `metagenomic_wgs_metadata`: 973K documents - WGS-specific metadata

#### 3. gold_metadata (~87MB)
- `biosamples`: 217K documents - GOLD biosample data
- `projects`: 221K documents - GOLD projects
- `studies`: 4,697 documents - GOLD studies

#### 4. nmdc (~1.6GB)
- `biosample_set`: 9,349 documents - NMDC biosamples
- `functional_annotation_agg`: 20M documents - Functional annotations
- `data_object_set`: 112K documents - NMDC data objects
- `workflow_execution_set`: 13K documents - Workflow executions
- Plus 15+ additional collections

#### 5. misc_metadata (~5MB)
- `submission_biosample_rows`: 19K documents - Submission data
- `nmdc_submissions`: 321 documents - NMDC submission metadata

### Local vs Load Balancer Differences

**Structural Differences**:
- `compact_mined_triads`: Present in loadbalancer (43M docs), missing locally
- SRA organization: Loadbalancer uses ncbi_metadata.sra_metadata, local uses separate biosamples database
- Analysis collections: Local has more intermediate/analytical collections
- Size: Loadbalancer ncbi_metadata is ~2x larger (50GB vs 25GB)

**Use Cases**:
- **Local**: Development, analysis, experimentation
- **Loadbalancer**: Production queries, stable data access

### MongoDB Operations & Performance

**Bulk Operations**:
- `make purge`: Drops entire `ncbi_metadata` database
- Individual collection drop: `mongosh --eval "db.getSiblingDB('ncbi_metadata').collection_name.drop()"`
- Most scripts don't clear collections before inserting (append mode)

**BioProjects Loading**:
- Complete XML file (~893K records) processes in ~15 minutes
- Oversized documents (>16MB) saved to `local/oversize/projects/` and `local/oversize/submissions/`
- Only 3 oversized documents out of ~893K records

**Indexing**:
See `mongo-js/` directory for index creation scripts:
- `biosamples_attributes`: unit, harmonized_name, attribute_name
- `env_triads_flattened`: accession, component, value_uc, curie
- `sra_biosamples_bioprojects`: biosample_id, bioproject_id
- Many collections have compound indexes for performance

---

## DuckDB & SQLite Usage

### Primary DuckDB Files

**ncbi_biosamples.duckdb**:
- Generated by `mongodb_biosamples_to_duckdb.py`
- Path: `local/ncbi_biosamples.duckdb`
- Created by: `make duck-all` in `ncbi_metadata.Makefile`

**Date-stamped versions**:
- `ncbi_biosamples_YYYY-MM-DD.duckdb`
- Used in notebooks for reproducibility
- Latest available at: https://portal.nersc.gov/project/m3408/biosamples_duckdb/

### DuckDB Tables

**Core Tables**:
- `normalized_context_strings`: Normalized environmental context values
- `curies_asserted`: CURIEs extracted via regex
- `curies_ner`: CURIEs identified via text annotation
- `curie_free_strings`: Context strings with CURIEs removed
- `ncbi_package_info`: NCBI biosample package metadata

**Analytical Tables**:
- `etl_log`: Processing metadata, download dates
- `ontology_class_candidates`: CURIEs for environmental context options
- `normalized_context_content_count`: Frequency counts

**Export Tables** (from MongoDB):
- Flat versions of MongoDB collections for SQL analysis
- Generated via `Makefiles/ncbi_to_duckdb.Makefile`

### SQLite Usage

**OAK Ontology Adapters**:
- `sqlite:obo:envo`: Environmental Ontology
- `sqlite:obo:po`: Plant Ontology
- Used for text annotation and CURIE lookup

**Local Ontology Files**:
- `local/envo.db`: Environmental Ontology database
- `local/goldterms.db`: GOLD ontology database

**Caching**:
- `requests_cache.sqlite`: Web request cache for BioPortal/OLS API calls
- Various lexical matching caches

### Database Technology Strategy

**MongoDB** (Primary Storage):
- Biosample storage with environmental context values
- All raw and processed data
- Future emphasis: in-MongoDB environmental triad mapping

**DuckDB** (Analytical Processing):
- Voting sheet generation
- SQL-style analysis on biosample data
- Export/sharing format (compressed, portable)

**SQLite** (Ontology Access):
- OAK adapters for ontology lookups
- Read-only access to ontologies

**PostgreSQL** (Deprecated):
- No longer used (see closed issue #25)
- Replaced by MongoDB-centered processing

---

## Data Flow & Processing Pipeline

### From Source to Analysis

```
1. DOWNLOAD
   ├─> NCBI FTP: biosample_set.xml.gz (~3GB, 44M biosamples)
   ├─> NCBI FTP: bioproject.xml (~3GB, 893K bioprojects)
   └─> BigQuery: nih-sra-datastore.sra.metadata (35M records)

2. LOAD TO MONGODB
   ├─> xml-to-mongo → ncbi_metadata.biosamples
   ├─> load_acceptable_sized_leaf_bioprojects_into_mongodb → ncbi_metadata.bioprojects
   └─> filtered_sra_metadata_to_mongo.ipynb → ncbi_metadata.filtered_sra_metadata

3. FLATTEN & EXTRACT
   ├─> flatten_biosample_attributes.js → biosamples_attributes
   ├─> flatten_env_triads_multi_component.js → env_triads_flattened
   └─> Various mongo-js/ scripts → analytical collections

4. ANNOTATE & NORMALIZE
   ├─> OAK text annotation → CURIE extraction
   ├─> BioPortal mapping → ontology term resolution
   └─> Normalization scripts → standardized formats

5. EXPORT TO DUCKDB
   └─> mongodb_biosamples_to_duckdb → ncbi_biosamples.duckdb

6. GENERATE VOTING SHEETS
   └─> generate_voting_sheet.ipynb → TSV files for review

7. INTEGRATE WITH SUBMISSION-SCHEMA
   └─> Voted terms → LinkML enumerations (via PR)
```

### Key Transformations

**XML → MongoDB**:
- Nested XML preserved as nested documents
- Oversized documents (>16MB) saved as files

**MongoDB → DuckDB**:
- Flattened structure for SQL analysis
- Normalized columns for dates, coordinates
- CURIE extraction and annotation results

**Environmental Triads Workflow**:
1. Extract from biosamples (env_broad_scale, env_local_scale, env_medium)
2. Normalize to remove variations
3. Annotate with OAK to extract CURIEs
4. Map via BioPortal for additional ontology terms
5. Generate voting sheets for domain expert review
6. Compile votes into value sets
7. Inject into submission-schema as enumerations

---

## External Data Sources

### NCBI Repositories
- **BioSamples XML**: `ftp.ncbi.nlm.nih.gov/biosample/biosample_set.xml.gz`
- **BioProjects XML**: `ftp.ncbi.nlm.nih.gov/bioproject/bioproject.xml`
- **Package Definitions**: `ncbi.nlm.nih.gov/biosample/docs/packages/?format=xml`
- **E-utilities API**: `eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi`

### BigQuery
- **SRA Metadata**: `nih-sra-datastore.sra.metadata` (35M+ records)
- Accessed via Google BigQuery client with application default credentials
- Filtered and imported to MongoDB for offline analysis

### Ontology Sources
- **OBO Foundry**: Via OAK adapters (`sqlite:obo:envo`, `sqlite:obo:po`)
- **BioPortal API**: `data.bioontology.org` (requires API key in `.env`)
- **MIxS Schema**: `raw.githubusercontent.com/GenomicsStandardsConsortium/mixs`

### Other Sources
- **GOLD Data**: `gold.jgi.doe.gov/download?mode=site_excel` (Excel format)
- **Unpaywall API**: `api.unpaywall.org/v2/{doi}` (for open access PDFs)

### Fetching Tools
- **wget**: Large FTP XML files
- **curl**: GOLD data, smaller API calls
- **requests**: Python API calls (NCBI E-utils, BioPortal, Unpaywall)
- **Google BigQuery client**: SRA metadata

### Caching Strategy
- OAK: Built-in SQLite caching for ontology lookups
- BioPortal: requests-cache SQLite database
- No automated refresh schedules (manual updates with timestamps in etl_log)

---

## Connection Patterns

### MongoDB Connection Methods

**Current State** (Inconsistent - see [Issue #176](https://github.com/microbiomedata/external-metadata-awareness/issues/176)):

1. **Component-Based** (host/port parameters):
   ```python
   MongoClient(host='localhost', port=27017)
   ```
   Used in: `xml_to_mongo.py`, `sra_accession_pairs_tsv_to_mongo.py`

2. **URI-Style** (connection string):
   ```python
   MongoClient('mongodb://localhost:27017/ncbi_metadata')
   ```
   Used in: `mongodb_biosamples_to_duckdb.py`, `load_acceptable_sized_leaf_bioprojects_into_mongodb.py`

3. **Utility Function** (via `mongodb_connection.py`):
   ```python
   from mongodb_connection import get_mongo_client
   client = get_mongo_client(mongo_uri='...', env_file='...', debug=True)
   ```
   Handles authentication, .env file loading
   Not consistently used across codebase

**Parameter Naming Inconsistency**:
- Some scripts: `--mongo_uri` (underscores)
- Other scripts: `--mongo-uri` (hyphens)
- Makefiles: `MONGO_HOST`, `MONGO_PORT`, `MONGO_DB`

**Authentication Handling**:
- Most scripts: No authentication (localhost only)
- Some notebooks: `.env` files with `NMDC_MONGO_USER`, `NMDC_MONGO_PASSWORD`
- `mongodb_connection.py`: Proper auth with `MONGO_USER`, `MONGO_PASSWORD`

**Target State** (from Issue #223):
1. All scripts use `mongodb_connection.py` utility
2. Unified .env file format
3. Consistent parameter naming
4. URI-style connections everywhere

### Makefile Variables

**Common Patterns**:
```makefile
MONGO_HOST ?= localhost
MONGO_PORT ?= 27017
MONGO_DB ?= ncbi_metadata
MONGO_URI ?= mongodb://$(MONGO_HOST):$(MONGO_PORT)/$(MONGO_DB)
```

**Usage in Scripts**:
```bash
poetry run script-name --mongo-uri $(MONGO_URI)
```

---

## Performance Considerations

### MongoDB
- **Indexes**: Critical for large collections (44M biosamples)
- **Batch Size**: Most scripts use 1k-10k record batches
- **Aggregation Pipelines**: Used for complex transformations
- **Memory**: Large aggregations may require `allowDiskUse: true`

### DuckDB
- **Compression**: .duckdb.gz files are ~50% of raw size
- **Query Performance**: Much faster than MongoDB for analytical queries
- **Memory Usage**: Entire database fits in memory on modern machines
- **Portability**: Single file, easy to share/archive

### Data Volume Scale
- **NCBI Biosamples**: 44M records, ~117GB raw in MongoDB
- **SRA Metadata**: 35M records, ~108GB raw
- **BioProjects**: 893K records, ~2.4GB raw
- **Total Storage**: ~150-200GB across all databases

---

## File Organization

### Data Directories (gitignored)
- `local/`: Generated data, DuckDB files, exports
- `local/oversize/`: MongoDB documents >16MB
- `downloads/`: Downloaded XML/data files

### Scripts by Function
- `external_metadata_awareness/`: Python scripts (54 files)
- `mongo-js/`: MongoDB JavaScript aggregation pipelines
- `Makefiles/`: Automated workflows
- `notebooks/`: Jupyter analysis notebooks

### Output Locations
- Voting sheets: `notebooks/environmental_context_value_sets/voting_sheets_output/`
- DuckDB exports: `local/ncbi_biosamples*.duckdb`
- CSV exports: `local/*.csv`

---

## References

- **CLAUDE.md**: Command reference and key concepts
- **GETTING_STARTED.md**: Quick start guide
- **PRIORITY_ROADMAP.md**: Current work priorities
- **Issue #176**: MongoDB connection unification
- **Issue #223**: Infrastructure improvements tracking
